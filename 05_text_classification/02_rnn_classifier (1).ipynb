{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbcIPWYG4Qj6"
   },
   "source": [
    "# RNN기반 분류기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1694,
     "status": "ok",
     "timestamp": 1750140631111,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "6U_oVjNc4c3I",
    "outputId": "339f9151-f4ae-4015-8be5-784639f4fc96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comp.graphics', 'rec.sport.baseball', 'sci.space']\n",
      "From: kjenks@gothamcity.jsc.nasa.gov\n",
      "Subject: Life on Mars???\n",
      "Organization: NASA/JSC/GM2, Space Shuttle Program Office \n",
      "X-Newsreader: TIN [version 1.1 PL8]\n",
      "Lines: 12\n",
      "\n",
      "I know it's only wishful thinking, with our current President,\n",
      "but this is from last fall:\n",
      "\n",
      "     \"Is there life on Mars?  Maybe not now.  But there will be.\"\n",
      "        -- Daniel S. Goldin, NASA Administrator, 24 August 1992\n",
      "\n",
      "-- Ken Jenks, NASA/JSC/GM2, Space Shuttle Program Office\n",
      "      kjenks@gothamcity.jsc.nasa.gov  (713) 483-4368\n",
      "\n",
      "     \"The man who makes no mistakes does not usually make\n",
      "      anything.\"\n",
      "        -- Edward John Phelps, American Diplomat/Lawyer (1825-1895)\n",
      "\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로딩\n",
    "from sklearn.datasets import fetch_20newsgroups     # 20 Newsgroups 텍스트 분류 데이터셋 로드\n",
    "\n",
    "categories = ['comp.graphics', 'sci.space', 'rec.sport.baseball']  # 사용할 뉴스그룹 카테고리 선택\n",
    "newsgroups = fetch_20newsgroups(                                  # 선택한 카테고리만 데이터 로드\n",
    "    subset='all',\n",
    "    categories=categories\n",
    ")\n",
    "\n",
    "X = newsgroups.data                           # 뉴스 문서 텍스트 리스트\n",
    "y = newsgroups.target                         # 각 문서의 클래스 인덱스 라벨\n",
    "\n",
    "print(newsgroups.target_names)                # 클래스 인덱스 → 실제 뉴스그룹 이름\n",
    "print(X[0])                                   # 첫 번째 뉴스 문서 원문 출력\n",
    "print(y[0])                                   # 첫 번째 문서의 클래스 인덱스 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6431,
     "status": "ok",
     "timestamp": 1750140647214,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "yXBtkCIp4H51",
    "outputId": "e593d05d-06d7-4379-efa1-6ddb83765ef1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2954, 200)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer        # 텍스트를 정수 시퀀스로 변환\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences # 시퀀스 길이 맞추기용 패딩 함수\n",
    "\n",
    "vocab_size = 10000                                               # 사용할 최대 단어 사전 크기\n",
    "max_len = 200                                                    # 모든 문장의 최대 길이\n",
    "\n",
    "tokenizer = Tokenizer(                                          # Tokenizer 객체 생성\n",
    "    num_words=vocab_size,                                       # 상위 vocab_size 단어만 사용\n",
    "    oov_token='<OOV>'                                           # 미등록 단어 토큰 설정\n",
    ")\n",
    "tokenizer.fit_on_texts(X)                                       # 전체 텍스트 기준으로 단어 사전 생성\n",
    "X_encoded = tokenizer.texts_to_sequences(X)                     # 문장을 정수 인덱스 시퀀스로 변환\n",
    "X_padded = pad_sequences(                                       # 모든 시퀀스를 동일한 길이로 패딩\n",
    "    X_encoded,\n",
    "    maxlen=max_len\n",
    ")\n",
    "\n",
    "print(X_padded.shape)                                           # (문서 수, max_len) 형태 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0suBZZ3V6ZuL"
   },
   "outputs": [],
   "source": [
    "# 데이터 분리 / 텐서 변환\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split     # 데이터 분할 함수\n",
    "from torch.utils.data import TensorDataset, DataLoader   # PyTorch Dataset / DataLoader\n",
    "\n",
    "# train / test 분리 (80% / 20%)\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(\n",
    "        torch.tensor(X_padded, dtype=torch.long),        # 패딩된 입력 데이터를 Long 텐서로 변환\n",
    "        torch.tensor(y, dtype=torch.long),               # 라벨 데이터를 텐서로 변환\n",
    "        test_size=0.2,                                   # 테스트 데이터 비율\n",
    "        random_state=42                                  # 재현성 확보\n",
    "    )\n",
    "\n",
    "# train / validation 분리 (train의 20%를 validation으로 사용)\n",
    "X_train, X_val, y_train, y_val = \\\n",
    "    train_test_split(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        test_size=0.2,                                   # 검증 데이터 비율\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "# Dataset 생성 (입력, 라벨 묶기)\n",
    "train_dataset = TensorDataset(X_train, y_train)          # 학습 데이터셋\n",
    "val_dataset = TensorDataset(X_val, y_val)                # 검증 데이터셋\n",
    "test_dataset = TensorDataset(X_test, y_test)             # 테스트 데이터셋\n",
    "\n",
    "# DataLoader 설정\n",
    "batch_size = 64                                          # 배치 크기\n",
    "train_loader = DataLoader(                               # 학습용 DataLoader\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True                                         # 학습 데이터는 셔플\n",
    ")\n",
    "val_loader = DataLoader(                                 # 검증용 DataLoader\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "test_loader = DataLoader(                                # 테스트용 DataLoader\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "y7BjT5FY8Tnt"
   },
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "import torch.nn as nn                              # PyTorch 신경망 모듈\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super().__init__()                         # nn.Module 초기화\n",
    "        # embedding → lstm → dense 구조\n",
    "        self.embedding = nn.Embedding(             # 단어 인덱스를 임베딩 벡터로 변환\n",
    "            vocab_size,                            # 단어 사전 크기\n",
    "            embedding_dim,                         # 임베딩 벡터 차원\n",
    "            padding_idx=0                          # PAD 토큰은 학습에 영향 없음\n",
    "        )\n",
    "        self.lstm = nn.LSTM(                       # 문맥 정보를 학습하는 LSTM\n",
    "            embedding_dim,                         # 입력 차원 (임베딩 크기)\n",
    "            hidden_size,                           # 은닉 상태 차원\n",
    "            batch_first=True                       # (batch, seq, feature) 형태 사용\n",
    "        )\n",
    "        self.fc = nn.Linear(                       # 최종 분류용 선형 레이어\n",
    "            hidden_size,                           # LSTM 은닉 상태 차원\n",
    "            3                                      # 클래스 개수 (3개 뉴스그룹)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)                      # (batch, seq) → (batch, seq, embed)\n",
    "        _, (h, c) = self.lstm(x)                   # LSTM 통과 (h: 마지막 은닉 상태)\n",
    "        out = self.fc(h[-1])                       # 마지막 타임스텝 은닉 상태로 분류\n",
    "        return out                                 # (batch, 3) 로짓 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11441,
     "status": "ok",
     "timestamp": 1750139218556,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "-vO1L30O8Vf5",
    "outputId": "3485c91a-9816-4db2-9410-6c819ee4562a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: Train Loss 1.0523, Train Acc 0.4735, Val Loss 0.9991, Val Acc 0.5137, \n",
      "Epoch 2/50: Train Loss 0.8993, Train Acc 0.6053, Val Loss 0.8621, Val Acc 0.6004, \n",
      "Epoch 3/50: Train Loss 0.6843, Train Acc 0.7196, Val Loss 0.7738, Val Acc 0.6596, \n",
      "Epoch 4/50: Train Loss 0.5067, Train Acc 0.8085, Val Loss 0.7244, Val Acc 0.6829, \n",
      "Epoch 5/50: Train Loss 0.3619, Train Acc 0.8651, Val Loss 0.6059, Val Acc 0.7590, \n",
      "Epoch 6/50: Train Loss 0.2494, Train Acc 0.9143, Val Loss 0.5693, Val Acc 0.7886, \n",
      "Epoch 7/50: Train Loss 0.1480, Train Acc 0.9519, Val Loss 0.5919, Val Acc 0.7970, \n",
      "Epoch 8/50: Train Loss 0.1380, Train Acc 0.9587, Val Loss 0.5525, Val Acc 0.7992, \n",
      "Epoch 9/50: Train Loss 0.1050, Train Acc 0.9635, Val Loss 0.5694, Val Acc 0.7886, \n",
      "Epoch 10/50: Train Loss 0.0610, Train Acc 0.9841, Val Loss 0.5322, Val Acc 0.8182, \n",
      "Epoch 11/50: Train Loss 0.0433, Train Acc 0.9899, Val Loss 0.6239, Val Acc 0.8372, \n",
      "Epoch 12/50: Train Loss 0.0270, Train Acc 0.9952, Val Loss 0.5573, Val Acc 0.8414, \n",
      "Epoch 13/50: Train Loss 0.0108, Train Acc 1.0000, Val Loss 0.6273, Val Acc 0.8140, \n",
      "Epoch 14/50: Train Loss 0.0102, Train Acc 0.9995, Val Loss 0.6378, Val Acc 0.8266, \n",
      "Epoch 15/50: Train Loss 0.0071, Train Acc 0.9995, Val Loss 0.7017, Val Acc 0.8140, \n",
      "Epoch 16/50: Train Loss 0.0046, Train Acc 1.0000, Val Loss 0.7605, Val Acc 0.8055, \n",
      "Epoch 17/50: Train Loss 0.0035, Train Acc 1.0000, Val Loss 0.6929, Val Acc 0.8288, \n",
      "Epoch 18/50: Train Loss 0.0467, Train Acc 0.9873, Val Loss 0.7599, Val Acc 0.7886, \n",
      "Epoch 19/50: Train Loss 0.0547, Train Acc 0.9852, Val Loss 0.6300, Val Acc 0.8288, \n",
      "Epoch 20/50: Train Loss 0.0973, Train Acc 0.9725, Val Loss 0.8063, Val Acc 0.7294, \n",
      "Epoch 21/50: Train Loss 0.1252, Train Acc 0.9646, Val Loss 0.6997, Val Acc 0.7548, \n",
      "Epoch 22/50: Train Loss 0.0246, Train Acc 0.9963, Val Loss 0.6334, Val Acc 0.8076, \n",
      "Epoch 23/50: Train Loss 0.0110, Train Acc 0.9984, Val Loss 0.6195, Val Acc 0.8097, \n",
      "Epoch 24/50: Train Loss 0.0052, Train Acc 1.0000, Val Loss 0.6101, Val Acc 0.8457, \n",
      "Epoch 25/50: Train Loss 0.0023, Train Acc 1.0000, Val Loss 0.6264, Val Acc 0.8309, \n",
      "Epoch 26/50: Train Loss 0.0016, Train Acc 1.0000, Val Loss 0.6467, Val Acc 0.8372, \n",
      "Epoch 27/50: Train Loss 0.0013, Train Acc 1.0000, Val Loss 0.6627, Val Acc 0.8414, \n",
      "Epoch 28/50: Train Loss 0.0011, Train Acc 1.0000, Val Loss 0.6804, Val Acc 0.8414, \n",
      "Epoch 29/50: Train Loss 0.0009, Train Acc 1.0000, Val Loss 0.6906, Val Acc 0.8457, \n",
      "Epoch 30/50: Train Loss 0.0008, Train Acc 1.0000, Val Loss 0.7025, Val Acc 0.8499, \n",
      "Epoch 31/50: Train Loss 0.0007, Train Acc 1.0000, Val Loss 0.7115, Val Acc 0.8499, \n",
      "Epoch 32/50: Train Loss 0.0006, Train Acc 1.0000, Val Loss 0.7261, Val Acc 0.8541, \n",
      "Epoch 33/50: Train Loss 0.0006, Train Acc 1.0000, Val Loss 0.7313, Val Acc 0.8541, \n",
      "Epoch 34/50: Train Loss 0.0005, Train Acc 1.0000, Val Loss 0.7394, Val Acc 0.8584, \n",
      "Epoch 35/50: Train Loss 0.0005, Train Acc 1.0000, Val Loss 0.7492, Val Acc 0.8605, \n",
      "Epoch 36/50: Train Loss 0.0004, Train Acc 1.0000, Val Loss 0.7565, Val Acc 0.8626, \n",
      "Epoch 37/50: Train Loss 0.0004, Train Acc 1.0000, Val Loss 0.7628, Val Acc 0.8626, \n",
      "Epoch 38/50: Train Loss 0.0004, Train Acc 1.0000, Val Loss 0.7651, Val Acc 0.8647, \n",
      "Epoch 39/50: Train Loss 0.0003, Train Acc 1.0000, Val Loss 0.7724, Val Acc 0.8605, \n",
      "Epoch 40/50: Train Loss 0.0003, Train Acc 1.0000, Val Loss 0.7777, Val Acc 0.8605, \n",
      "Epoch 41/50: Train Loss 0.0003, Train Acc 1.0000, Val Loss 0.7841, Val Acc 0.8605, \n",
      "Epoch 42/50: Train Loss 0.0003, Train Acc 1.0000, Val Loss 0.7888, Val Acc 0.8626, \n",
      "Epoch 43/50: Train Loss 0.0003, Train Acc 1.0000, Val Loss 0.7946, Val Acc 0.8647, \n",
      "Epoch 44/50: Train Loss 0.0003, Train Acc 1.0000, Val Loss 0.8007, Val Acc 0.8626, \n",
      "Epoch 45/50: Train Loss 0.0002, Train Acc 1.0000, Val Loss 0.8063, Val Acc 0.8626, \n",
      "Epoch 46/50: Train Loss 0.0002, Train Acc 1.0000, Val Loss 0.8122, Val Acc 0.8626, \n",
      "Epoch 47/50: Train Loss 0.0002, Train Acc 1.0000, Val Loss 0.8255, Val Acc 0.8584, \n",
      "Epoch 48/50: Train Loss 0.0002, Train Acc 1.0000, Val Loss 0.8285, Val Acc 0.8520, \n",
      "Epoch 49/50: Train Loss 0.0002, Train Acc 1.0000, Val Loss 0.8385, Val Acc 0.8499, \n",
      "Epoch 50/50: Train Loss 0.0002, Train Acc 1.0000, Val Loss 0.8471, Val Acc 0.8520, \n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "import torch.optim as optim                                  # 옵티마이저 모듈\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # cuda 가능하면 GPU 사용\n",
    "embedding_dim = 100                                          # 임베딩 벡터 차원\n",
    "hidden_size = 128                                            # LSTM 은닉 상태 차원\n",
    "\n",
    "model = LSTMClassifier(vocab_size, embedding_dim, hidden_size).to(device)  # 모델 생성 후 GPU 이동\n",
    "criterion = nn.CrossEntropyLoss()                            # 다중 클래스 분류용 손실 함수\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)         # Adam 옵티마이저 설정\n",
    "\n",
    "# 학습 루프 기록용\n",
    "train_losses, train_accs = [], []                            # 학습 손실/정확도 기록\n",
    "val_losses, val_accs = [], []                                # 검증 손실/정확도 기록\n",
    "\n",
    "epochs = 50                                                  # 전체 학습 에폭 수\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # 학습\n",
    "    model.train()                                            # 학습 모드 전환\n",
    "    train_loss, train_correct, train_total = 0, 0, 0          # 에폭 누적 변수 초기화\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # 배치를 GPU로 이동\n",
    "        optimizer.zero_grad()                                # 이전 gradient 초기화\n",
    "        output = model(X_batch)                              # 순전파 (로짓 출력)\n",
    "        loss = criterion(output, y_batch)                    # 손실 계산\n",
    "        loss.backward()                                      # 역전파\n",
    "        optimizer.step()                                     # 파라미터 업데이트\n",
    "\n",
    "        train_loss += loss.detach().cpu().item()             # 배치 손실 누적\n",
    "        pred = output.argmax(dim=1)                          # 가장 큰 로짓 인덱스 = 예측 클래스\n",
    "        train_correct += (pred == y_batch).sum().detach().cpu().item()  # 정답 개수 누적\n",
    "        train_total += len(y_batch)                          # 전체 샘플 수 누적\n",
    "\n",
    "    train_loss /= len(train_loader)                          # 에폭 평균 학습 손실\n",
    "    train_acc = train_correct / train_total                  # 에폭 학습 정확도\n",
    "    train_losses.append(train_loss)                          # 학습 손실 기록\n",
    "    train_accs.append(train_acc)                             # 학습 정확도 기록\n",
    "\n",
    "    # 검증\n",
    "    model.eval()                                             # 평가 모드 전환\n",
    "    val_loss, val_correct, val_total = 0, 0, 0                # 검증 누적 변수 초기화\n",
    "\n",
    "    with torch.no_grad():                                    # gradient 계산 비활성화\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # 배치를 GPU로 이동\n",
    "            output = model(X_batch)                           # 검증 순전파\n",
    "            loss = criterion(output, y_batch)                 # 검증 손실 계산\n",
    "\n",
    "            val_loss += loss.detach().cpu().item()            # 검증 손실 누적\n",
    "            pred = output.argmax(dim=1)                       # 예측 클래스\n",
    "            val_correct += (pred == y_batch).sum().detach().cpu().item()  # 정답 누적\n",
    "            val_total += len(y_batch)                         # 샘플 수 누적\n",
    "\n",
    "        val_loss /= len(val_loader)                           # 에폭 평균 검증 손실\n",
    "        val_acc = val_correct / val_total                     # 에폭 검증 정확도\n",
    "        val_losses.append(val_loss)                           # 검증 손실 기록\n",
    "        val_accs.append(val_acc)                              # 검증 정확도 기록\n",
    "\n",
    "    # 출력 (train_loss, val_loss)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}: '                     # 현재 에폭 정보 출력\n",
    "          f'Train Loss {train_loss:.4f}, '\n",
    "          f'Train Acc {train_acc:.4f}, '\n",
    "          f'Val Loss {val_loss:.4f}, '\n",
    "          f'Val Acc {val_acc:.4f}, ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1750139872430,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "clNCNefO9nBv",
    "outputId": "4505f72f-01c6-4123-81bc-30dc2c86a93c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     comp.graphics       0.82      0.85      0.83       202\n",
      "rec.sport.baseball       0.86      0.92      0.89       202\n",
      "         sci.space       0.85      0.77      0.81       187\n",
      "\n",
      "          accuracy                           0.85       591\n",
      "         macro avg       0.85      0.84      0.84       591\n",
      "      weighted avg       0.85      0.85      0.85       591\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가\n",
    "# - 정답 라벨과 모델 예측값을 사용해 classification_report 생성\n",
    "from sklearn.metrics import classification_report    # 분류 성능 리포트 함수\n",
    "\n",
    "model.eval()                                         # 모델을 평가 모드로 전환\n",
    "all_preds, all_labels = [], []                       # 전체 예측값/정답 저장용 리스트\n",
    "\n",
    "with torch.no_grad():                                # gradient 계산 비활성화\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # 배치를 GPU로 이동\n",
    "        output = model(X_batch)                      # 모델 예측값(로짓) 계산\n",
    "        loss = criterion(output, y_batch)            # 테스트 손실 계산(로그용)\n",
    "        pred = output.argmax(dim=1)                  # 가장 큰 로짓을 갖는 클래스 선택\n",
    "\n",
    "        all_preds.extend(pred.detach().cpu().numpy())    # 예측 결과를 CPU numpy로 저장\n",
    "        all_labels.extend(y_batch.detach().cpu().numpy())# 실제 라벨 저장\n",
    "\n",
    "print(\n",
    "    classification_report(                           # 클래스별 성능 지표 출력\n",
    "        all_labels,                                  # 실제 라벨\n",
    "        all_preds,                                   # 예측 라벨\n",
    "        target_names=newsgroups.target_names          # 클래스 이름 매핑\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRcaUXF_gGi-"
   },
   "source": [
    "## 사전학습된 임베딩 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22000,
     "status": "ok",
     "timestamp": 1750140554021,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "a9UBkjQlgQdh",
    "outputId": "2f5e4a97-1014-4754-f96f-9a54a3da4027"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gensim -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1440,
     "status": "ok",
     "timestamp": 1750140871457,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "w8s1Ezq6hQlW",
    "outputId": "da2ef527-897d-4b87-ae81-c7ba2bb5980d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText                 # FastText 단어 임베딩 모델 클래스\n",
    "\n",
    "fasttext_model = FastText.load('ted_en_fasttext.model')  # 사전 학습된 FastText 모델 로드\n",
    "print(fasttext_model.vector_size)                  # 각 단어를 표현하는 임베딩 벡터 차원 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 104,
     "status": "ok",
     "timestamp": 1750141482707,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "bv-YDGQJhyTf",
    "outputId": "aa62b304-44f1-45b2-c9ac-3a1094b38e5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np                                      # 수치 계산용 numpy\n",
    "\n",
    "embedding_dim = fasttext_model.vector_size              # FastText 임베딩 벡터 차원\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))# 단어 사전 크기 × 임베딩 차원 행렬 생성\n",
    "\n",
    "word_index = tokenizer.word_index                       # 전체 단어 → 인덱스 사전 (예: 38,000개)\n",
    "word_index = {word: index                               # vocab_size 이내 단어만 필터링\n",
    "              for word, index in word_index.items()\n",
    "              if index < vocab_size}\n",
    "print(len(word_index))                                  # 실제 사용할 단어 수 확인 (예: 10,000)\n",
    "\n",
    "for word, index in word_index.items():                  # 단어 사전 순회\n",
    "    if word in fasttext_model.wv:                       # FastText 모델에 단어가 있으면\n",
    "        embedding_matrix[index] = fasttext_model.wv[word]  # 해당 단어 임베딩 벡터 복사\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-wiQxp4Ag9xz"
   },
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "import torch.nn as nn                               # PyTorch 신경망 모듈\n",
    "\n",
    "class LSTMClassifier2(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, embedding_matrix, hidden_size):\n",
    "        super().__init__()                          # nn.Module 초기화\n",
    "        # embedding → lstm → dense 구조\n",
    "\n",
    "        self.embedding = nn.Embedding(              # 단어 인덱스를 임베딩 벡터로 변환\n",
    "            vocab_size,                             # 단어 사전 크기\n",
    "            embedding_dim,                          # 임베딩 벡터 차원\n",
    "            padding_idx=0                           # PAD 토큰은 학습 영향 없음\n",
    "        )\n",
    "        self.embedding.weight.data.copy_(            # 사전학습된 FastText 임베딩 가중치 복사\n",
    "            torch.from_numpy(embedding_matrix)\n",
    "        )\n",
    "        self.embedding.weight.requires_grad = True   # 임베딩을 미세조정(fine-tuning) 허용\n",
    "\n",
    "        self.lstm = nn.LSTM(                        # 문맥 정보를 학습하는 LSTM\n",
    "            embedding_dim,                          # 입력 차원 (임베딩 크기)\n",
    "            hidden_size,                            # 은닉 상태 차원\n",
    "            batch_first=True                        # (batch, seq, feature) 형태 사용\n",
    "        )\n",
    "        self.fc = nn.Linear(                        # 최종 분류용 선형 레이어\n",
    "            hidden_size,                            # LSTM 은닉 상태 차원\n",
    "            3                                       # 클래스 개수 (3개 뉴스그룹)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)                       # (batch, seq) → (batch, seq, embed)\n",
    "        _, (h, c) = self.lstm(x)                    # LSTM 통과 (h: 마지막 은닉 상태)\n",
    "        out = self.fc(h[-1])                        # 마지막 타임스텝 은닉 상태로 분류\n",
    "        return out                                  # (batch, 3) 로짓 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23895,
     "status": "ok",
     "timestamp": 1750142124548,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "47_k6ozcg9x1",
    "outputId": "c2d0c0d9-98bb-40f9-bf06-bf56a79d8627"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: Train Loss 1.0995, Train Acc 0.3254, Val Loss 1.0942, Val Acc 0.3679, \n",
      "Epoch 2/100: Train Loss 1.0955, Train Acc 0.3561, Val Loss 1.0909, Val Acc 0.4123, \n",
      "Epoch 3/100: Train Loss 1.0922, Train Acc 0.4333, Val Loss 1.0881, Val Acc 0.4334, \n",
      "Epoch 4/100: Train Loss 1.0881, Train Acc 0.4725, Val Loss 1.0841, Val Acc 0.4101, \n",
      "Epoch 5/100: Train Loss 1.0823, Train Acc 0.4963, Val Loss 1.0781, Val Acc 0.4482, \n",
      "Epoch 6/100: Train Loss 1.0711, Train Acc 0.5058, Val Loss 1.0623, Val Acc 0.5074, \n",
      "Epoch 7/100: Train Loss 1.0039, Train Acc 0.5556, Val Loss 0.9198, Val Acc 0.5603, \n",
      "Epoch 8/100: Train Loss 0.9900, Train Acc 0.4921, Val Loss 0.9496, Val Acc 0.5370, \n",
      "Epoch 9/100: Train Loss 0.8955, Train Acc 0.6233, Val Loss 0.8860, Val Acc 0.6110, \n",
      "Epoch 10/100: Train Loss 0.8439, Train Acc 0.6116, Val Loss 0.8962, Val Acc 0.5793, \n",
      "Epoch 11/100: Train Loss 0.8038, Train Acc 0.6577, Val Loss 0.8250, Val Acc 0.6512, \n",
      "Epoch 12/100: Train Loss 0.7747, Train Acc 0.6757, Val Loss 0.7869, Val Acc 0.6554, \n",
      "Epoch 13/100: Train Loss 0.6939, Train Acc 0.7344, Val Loss 0.7055, Val Acc 0.6871, \n",
      "Epoch 14/100: Train Loss 0.6432, Train Acc 0.7460, Val Loss 0.7141, Val Acc 0.6871, \n",
      "Epoch 15/100: Train Loss 0.5647, Train Acc 0.7931, Val Loss 0.6207, Val Acc 0.7590, \n",
      "Epoch 16/100: Train Loss 0.4936, Train Acc 0.8138, Val Loss 0.5210, Val Acc 0.7970, \n",
      "Epoch 17/100: Train Loss 0.4417, Train Acc 0.8429, Val Loss 0.4936, Val Acc 0.8055, \n",
      "Epoch 18/100: Train Loss 0.3972, Train Acc 0.8503, Val Loss 0.4809, Val Acc 0.8203, \n",
      "Epoch 19/100: Train Loss 0.3555, Train Acc 0.8804, Val Loss 0.5257, Val Acc 0.7738, \n",
      "Epoch 20/100: Train Loss 0.3025, Train Acc 0.8958, Val Loss 0.4467, Val Acc 0.8457, \n",
      "Epoch 21/100: Train Loss 0.2744, Train Acc 0.9111, Val Loss 0.3846, Val Acc 0.8562, \n",
      "Epoch 22/100: Train Loss 0.2716, Train Acc 0.9169, Val Loss 0.5082, Val Acc 0.8245, \n",
      "Epoch 23/100: Train Loss 0.2943, Train Acc 0.9048, Val Loss 0.4136, Val Acc 0.8372, \n",
      "Epoch 24/100: Train Loss 0.2233, Train Acc 0.9286, Val Loss 0.4048, Val Acc 0.8732, \n",
      "Epoch 25/100: Train Loss 0.1861, Train Acc 0.9455, Val Loss 0.4139, Val Acc 0.8689, \n",
      "Epoch 26/100: Train Loss 0.2083, Train Acc 0.9439, Val Loss 0.3964, Val Acc 0.8732, \n",
      "Epoch 27/100: Train Loss 0.2693, Train Acc 0.9169, Val Loss 0.3140, Val Acc 0.8879, \n",
      "Epoch 28/100: Train Loss 0.1664, Train Acc 0.9545, Val Loss 0.3020, Val Acc 0.9133, \n",
      "Epoch 29/100: Train Loss 0.1491, Train Acc 0.9635, Val Loss 0.3051, Val Acc 0.9049, \n",
      "Epoch 30/100: Train Loss 0.1294, Train Acc 0.9677, Val Loss 0.2677, Val Acc 0.9154, \n",
      "Epoch 31/100: Train Loss 0.2576, Train Acc 0.9206, Val Loss 0.3178, Val Acc 0.9006, \n",
      "Epoch 32/100: Train Loss 0.1382, Train Acc 0.9651, Val Loss 0.2747, Val Acc 0.9091, \n",
      "Epoch 33/100: Train Loss 0.1097, Train Acc 0.9720, Val Loss 0.2886, Val Acc 0.8985, \n",
      "Epoch 34/100: Train Loss 0.1051, Train Acc 0.9746, Val Loss 0.2588, Val Acc 0.9112, \n",
      "Epoch 35/100: Train Loss 0.0998, Train Acc 0.9772, Val Loss 0.2858, Val Acc 0.9133, \n",
      "Epoch 36/100: Train Loss 0.0901, Train Acc 0.9778, Val Loss 0.3011, Val Acc 0.9133, \n",
      "Epoch 37/100: Train Loss 0.0845, Train Acc 0.9815, Val Loss 0.2640, Val Acc 0.9133, \n",
      "Epoch 38/100: Train Loss 0.1556, Train Acc 0.9566, Val Loss 0.3103, Val Acc 0.8964, \n",
      "Epoch 39/100: Train Loss 0.0990, Train Acc 0.9725, Val Loss 0.2850, Val Acc 0.9197, \n",
      "Epoch 40/100: Train Loss 0.1605, Train Acc 0.9556, Val Loss 0.3628, Val Acc 0.8879, \n",
      "Epoch 41/100: Train Loss 0.1023, Train Acc 0.9720, Val Loss 0.3717, Val Acc 0.8858, \n",
      "Epoch 42/100: Train Loss 0.1292, Train Acc 0.9667, Val Loss 0.2416, Val Acc 0.9302, \n",
      "Epoch 43/100: Train Loss 0.0799, Train Acc 0.9815, Val Loss 0.3832, Val Acc 0.8943, \n",
      "Epoch 44/100: Train Loss 0.0697, Train Acc 0.9836, Val Loss 0.2695, Val Acc 0.9197, \n",
      "Epoch 45/100: Train Loss 0.2719, Train Acc 0.9354, Val Loss 1.1518, Val Acc 0.7019, \n",
      "Epoch 46/100: Train Loss 0.1934, Train Acc 0.9434, Val Loss 0.2620, Val Acc 0.9239, \n",
      "Epoch 47/100: Train Loss 0.0859, Train Acc 0.9825, Val Loss 0.2908, Val Acc 0.9175, \n",
      "Epoch 48/100: Train Loss 0.0714, Train Acc 0.9852, Val Loss 0.2662, Val Acc 0.9239, \n",
      "Epoch 49/100: Train Loss 0.0754, Train Acc 0.9820, Val Loss 0.2606, Val Acc 0.9175, \n",
      "Epoch 50/100: Train Loss 0.1036, Train Acc 0.9788, Val Loss 0.2549, Val Acc 0.9239, \n",
      "Epoch 51/100: Train Loss 0.0774, Train Acc 0.9836, Val Loss 0.2848, Val Acc 0.9049, \n",
      "Epoch 52/100: Train Loss 0.1026, Train Acc 0.9735, Val Loss 0.2179, Val Acc 0.9260, \n",
      "Epoch 53/100: Train Loss 0.0511, Train Acc 0.9910, Val Loss 0.2588, Val Acc 0.9197, \n",
      "Epoch 54/100: Train Loss 0.0510, Train Acc 0.9915, Val Loss 0.2375, Val Acc 0.9302, \n",
      "Epoch 55/100: Train Loss 0.0491, Train Acc 0.9915, Val Loss 0.2443, Val Acc 0.9260, \n",
      "Epoch 56/100: Train Loss 0.0401, Train Acc 0.9931, Val Loss 0.2539, Val Acc 0.9281, \n",
      "Epoch 57/100: Train Loss 0.0516, Train Acc 0.9905, Val Loss 0.2646, Val Acc 0.9239, \n",
      "Epoch 58/100: Train Loss 0.0715, Train Acc 0.9836, Val Loss 0.2088, Val Acc 0.9345, \n",
      "Epoch 59/100: Train Loss 0.0605, Train Acc 0.9884, Val Loss 0.2133, Val Acc 0.9429, \n",
      "Epoch 60/100: Train Loss 0.0404, Train Acc 0.9921, Val Loss 0.2215, Val Acc 0.9366, \n",
      "Epoch 61/100: Train Loss 0.0350, Train Acc 0.9937, Val Loss 0.2603, Val Acc 0.9302, \n",
      "Epoch 62/100: Train Loss 0.0400, Train Acc 0.9926, Val Loss 0.2522, Val Acc 0.9302, \n",
      "Epoch 63/100: Train Loss 0.0357, Train Acc 0.9937, Val Loss 0.2413, Val Acc 0.9345, \n",
      "Epoch 64/100: Train Loss 0.0312, Train Acc 0.9942, Val Loss 0.2538, Val Acc 0.9302, \n",
      "Epoch 65/100: Train Loss 0.0312, Train Acc 0.9942, Val Loss 0.2554, Val Acc 0.9302, \n",
      "Epoch 66/100: Train Loss 0.0291, Train Acc 0.9947, Val Loss 0.2802, Val Acc 0.9239, \n",
      "Epoch 67/100: Train Loss 0.0319, Train Acc 0.9931, Val Loss 0.2446, Val Acc 0.9302, \n",
      "Epoch 68/100: Train Loss 0.0317, Train Acc 0.9937, Val Loss 0.2676, Val Acc 0.9281, \n",
      "Epoch 69/100: Train Loss 0.3478, Train Acc 0.8979, Val Loss 0.6573, Val Acc 0.8076, \n",
      "Epoch 70/100: Train Loss 0.2561, Train Acc 0.9317, Val Loss 0.3604, Val Acc 0.8901, \n",
      "Epoch 71/100: Train Loss 0.1919, Train Acc 0.9434, Val Loss 0.4180, Val Acc 0.8964, \n",
      "Epoch 72/100: Train Loss 0.1361, Train Acc 0.9661, Val Loss 0.2914, Val Acc 0.9281, \n",
      "Epoch 73/100: Train Loss 0.0998, Train Acc 0.9725, Val Loss 0.2428, Val Acc 0.9323, \n",
      "Epoch 74/100: Train Loss 0.0583, Train Acc 0.9868, Val Loss 0.2215, Val Acc 0.9323, \n",
      "Epoch 75/100: Train Loss 0.0386, Train Acc 0.9921, Val Loss 0.2362, Val Acc 0.9429, \n",
      "Epoch 76/100: Train Loss 0.0778, Train Acc 0.9794, Val Loss 0.2533, Val Acc 0.9260, \n",
      "Epoch 77/100: Train Loss 0.0516, Train Acc 0.9894, Val Loss 0.1827, Val Acc 0.9471, \n",
      "Epoch 78/100: Train Loss 0.0286, Train Acc 0.9947, Val Loss 0.2323, Val Acc 0.9345, \n",
      "Epoch 79/100: Train Loss 0.0267, Train Acc 0.9942, Val Loss 0.2127, Val Acc 0.9387, \n",
      "Epoch 80/100: Train Loss 0.0244, Train Acc 0.9952, Val Loss 0.2177, Val Acc 0.9387, \n",
      "Epoch 81/100: Train Loss 0.0219, Train Acc 0.9958, Val Loss 0.2343, Val Acc 0.9408, \n",
      "Epoch 82/100: Train Loss 0.0201, Train Acc 0.9952, Val Loss 0.2524, Val Acc 0.9387, \n",
      "Epoch 83/100: Train Loss 0.0255, Train Acc 0.9947, Val Loss 0.2185, Val Acc 0.9408, \n",
      "Epoch 84/100: Train Loss 0.0477, Train Acc 0.9894, Val Loss 0.2376, Val Acc 0.9408, \n",
      "Epoch 85/100: Train Loss 0.0721, Train Acc 0.9836, Val Loss 0.1720, Val Acc 0.9535, \n",
      "Epoch 86/100: Train Loss 0.0244, Train Acc 0.9958, Val Loss 0.1904, Val Acc 0.9450, \n",
      "Epoch 87/100: Train Loss 0.0172, Train Acc 0.9974, Val Loss 0.2178, Val Acc 0.9387, \n",
      "Epoch 88/100: Train Loss 0.0178, Train Acc 0.9963, Val Loss 0.2181, Val Acc 0.9387, \n",
      "Epoch 89/100: Train Loss 0.0151, Train Acc 0.9979, Val Loss 0.2262, Val Acc 0.9387, \n",
      "Epoch 90/100: Train Loss 0.0167, Train Acc 0.9968, Val Loss 0.2395, Val Acc 0.9366, \n",
      "Epoch 91/100: Train Loss 0.0154, Train Acc 0.9974, Val Loss 0.2167, Val Acc 0.9366, \n",
      "Epoch 92/100: Train Loss 0.0154, Train Acc 0.9974, Val Loss 0.2209, Val Acc 0.9408, \n",
      "Epoch 93/100: Train Loss 0.0144, Train Acc 0.9979, Val Loss 0.2286, Val Acc 0.9387, \n",
      "Epoch 94/100: Train Loss 0.0140, Train Acc 0.9974, Val Loss 0.1978, Val Acc 0.9450, \n",
      "Epoch 95/100: Train Loss 0.0155, Train Acc 0.9968, Val Loss 0.2139, Val Acc 0.9387, \n",
      "Epoch 96/100: Train Loss 0.0145, Train Acc 0.9974, Val Loss 0.2353, Val Acc 0.9366, \n",
      "Epoch 97/100: Train Loss 0.0127, Train Acc 0.9979, Val Loss 0.2256, Val Acc 0.9429, \n",
      "Epoch 98/100: Train Loss 0.0134, Train Acc 0.9979, Val Loss 0.2289, Val Acc 0.9366, \n",
      "Epoch 99/100: Train Loss 0.0112, Train Acc 0.9984, Val Loss 0.2432, Val Acc 0.9387, \n",
      "Epoch 100/100: Train Loss 0.0109, Train Acc 0.9984, Val Loss 0.2501, Val Acc 0.9366, \n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "import torch.optim as optim                                  # 옵티마이저 모듈\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # cuda 가능하면 GPU 사용\n",
    "embedding_dim = 100                                          # 임베딩 벡터 차원\n",
    "hidden_size = 128                                            # LSTM 은닉 상태 차원\n",
    "\n",
    "model = LSTMClassifier2(vocab_size, embedding_dim, embedding_matrix, hidden_size).to(device)  # 모델 생성 후 GPU 이동\n",
    "criterion = nn.CrossEntropyLoss()                            # 다중 클래스 분류용 손실 함수\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)        # Adam 옵티마이저 설정 (학습률 1e-4)\n",
    "\n",
    "# 학습 루프 기록용\n",
    "train_losses, train_accs = [], []                            # 학습 손실/정확도 기록\n",
    "val_losses, val_accs = [], []                                # 검증 손실/정확도 기록\n",
    "\n",
    "epochs = 100                                                 # 전체 학습 에폭 수\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # 학습\n",
    "    model.train()                                            # 학습 모드 전환\n",
    "    train_loss, train_correct, train_total = 0, 0, 0          # 에폭 누적 변수 초기화\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # 배치를 GPU로 이동\n",
    "        optimizer.zero_grad()                                # 이전 gradient 초기화\n",
    "        output = model(X_batch)                              # 순전파 (클래스 로짓 출력)\n",
    "        loss = criterion(output, y_batch)                    # 손실 계산\n",
    "        loss.backward()                                      # 역전파\n",
    "        optimizer.step()                                     # 파라미터 업데이트\n",
    "\n",
    "        train_loss += loss.detach().cpu().item()             # 배치 손실 누적\n",
    "        pred = output.argmax(dim=1)                          # 예측 클래스(최대 로짓 인덱스)\n",
    "        train_correct += (pred == y_batch).sum().detach().cpu().item()  # 정답 개수 누적\n",
    "        train_total += len(y_batch)                          # 전체 샘플 수 누적\n",
    "\n",
    "    train_loss /= len(train_loader)                          # 에폭 평균 학습 손실\n",
    "    train_acc = train_correct / train_total                  # 에폭 학습 정확도\n",
    "    train_losses.append(train_loss)                          # 학습 손실 기록\n",
    "    train_accs.append(train_acc)                             # 학습 정확도 기록\n",
    "\n",
    "    # 검증\n",
    "    model.eval()                                             # 평가 모드 전환\n",
    "    val_loss, val_correct, val_total = 0, 0, 0                # 검증 누적 변수 초기화\n",
    "\n",
    "    with torch.no_grad():                                    # gradient 계산 비활성화\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # 배치를 GPU로 이동\n",
    "            output = model(X_batch)                           # 검증 순전파\n",
    "            loss = criterion(output, y_batch)                 # 검증 손실 계산\n",
    "\n",
    "            val_loss += loss.detach().cpu().item()            # 검증 손실 누적\n",
    "            pred = output.argmax(dim=1)                       # 예측 클래스\n",
    "            val_correct += (pred == y_batch).sum().detach().cpu().item()  # 정답 누적\n",
    "            val_total += len(y_batch)                         # 샘플 수 누적\n",
    "\n",
    "        val_loss /= len(val_loader)                           # 에폭 평균 검증 손실\n",
    "        val_acc = val_correct / val_total                     # 에폭 검증 정확도\n",
    "        val_losses.append(val_loss)                           # 검증 손실 기록\n",
    "        val_accs.append(val_acc)                              # 검증 정확도 기록\n",
    "\n",
    "    # 출력 (train_loss, val_loss)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}: '                     # 현재 에폭 정보 출력\n",
    "          f'Train Loss {train_loss:.4f}, '\n",
    "          f'Train Acc {train_acc:.4f}, '\n",
    "          f'Val Loss {val_loss:.4f}, '\n",
    "          f'Val Acc {val_acc:.4f}, ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1750142126818,
     "user": {
      "displayName": "sh qkel",
      "userId": "12967633263148061954"
     },
     "user_tz": -540
    },
    "id": "-JkT4PSYg9x3",
    "outputId": "95531598-aa4b-4ba6-daa1-f95780ffa7a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     comp.graphics       0.93      0.93      0.93       202\n",
      "rec.sport.baseball       0.95      0.95      0.95       202\n",
      "         sci.space       0.93      0.93      0.93       187\n",
      "\n",
      "          accuracy                           0.94       591\n",
      "         macro avg       0.94      0.94      0.94       591\n",
      "      weighted avg       0.94      0.94      0.94       591\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가\n",
    "# - 정답 라벨과 모델 예측값을 사용해 classification_report 생성\n",
    "from sklearn.metrics import classification_report    # 분류 성능 리포트 함수\n",
    "\n",
    "model.eval()                                         # 모델을 평가 모드로 전환\n",
    "all_preds, all_labels = [], []                       # 전체 예측값/정답 저장용 리스트\n",
    "\n",
    "with torch.no_grad():                                # gradient 계산 비활성화\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)  # 배치를 GPU로 이동\n",
    "        output = model(X_batch)                      # 모델 예측값(로짓) 계산\n",
    "        loss = criterion(output, y_batch)            # 테스트 손실 계산(로그/확인용)\n",
    "        pred = output.argmax(dim=1)                  # 가장 큰 로짓을 갖는 클래스 선택\n",
    "\n",
    "        all_preds.extend(pred.detach().cpu().numpy())    # 예측 라벨을 CPU numpy로 저장\n",
    "        all_labels.extend(y_batch.detach().cpu().numpy())# 실제 라벨을 CPU numpy로 저장\n",
    "\n",
    "print(\n",
    "    classification_report(                           # 클래스별 precision/recall/f1 출력\n",
    "        all_labels,                                  # 실제 라벨\n",
    "        all_preds,                                   # 예측 라벨\n",
    "        target_names=newsgroups.target_names          # 클래스 이름 매핑\n",
    "    )\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNEca1JB5mJdMj6HFe4GY4d",
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
