{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b7e722",
   "metadata": {},
   "source": [
    "# HuggingFace Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5483b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets\n",
    "!pip install -q sentencepiece\n",
    "!pip install -q kobert-transformers\n",
    "!pip install -q python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34a8fc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # ëª¨ë“  ê²½ê³  ë¬´ì‹œ\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv('HF_TOKEN')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84ccf32",
   "metadata": {},
   "source": [
    "### NLP Tasks\n",
    "\n",
    "ì£¼ì–´ì§„ taskì—ì„œ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê°€ì¥ ê°„ë‹¨í•œ ë°©ë²•ì€ `pipeline`ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "605f6bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a9af30",
   "metadata": {},
   "source": [
    "##### ê¸°ê³„ë²ˆì—­\n",
    "\n",
    "https://huggingface.co/Helsinki-NLP/opus-mt-ko-en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b2bd04",
   "metadata": {},
   "source": [
    "pipeline() : í•­ìƒ Hugging Face Hubì— ìˆëŠ” ëª¨ë¸ì„ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©\n",
    "- ëª¨ë¸ì„ ëª…ì‹œí•˜ì§€ ì•ŠëŠ” ê²½ìš°\n",
    "    - HuggingFaceê°€ taskë³„ ê¸°ë³¸(default) ëª¨ë¸ì„ ìë™ ì‚¬ìš©\n",
    "    - ì´ ê¸°ë³¸ëª¨ë¸ë“¤ì€ HuggingFace Hubì— ê³µê°œëœ ëª¨ë¸\n",
    "- ëª¨ë¸ì„ ëª…ì‹œí•œ ê²½ìš°\n",
    "    - HuggingFaceì— ì˜¬ë¼ì˜¨ íŠ¹ì • repoì—ì„œ ë‹¤ìš´ë¡œë“œ\n",
    "- ëª¨ë¸ ê°ì²´ë¥¼ ì§ì ‘ ë„˜ê¸¸ ê²½ìš°\n",
    "    - ë¡œì»¬ì—ì„œ ë¡œë“œ\n",
    "    - ì§ì ‘ í•™ìŠµ/íŒŒì¸íŠœë‹í•œ ëª¨ë¸\n",
    "    - ì´ëŸ° ê²½ìš°ì—ëŠ” pipelineì´ \"ì¶”ë¡  ë ˆí¬\"ì˜ ì—­í• ë§Œ í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ccd73e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "296d4d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text2text_generation.TranslationPipeline at 0x1de30238cb0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator = pipeline('translation', model='Helsinki-NLP/opus-mt-ko-en')\n",
    "translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "63f52e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Turn to the hot dog!'}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator('í•«ë„ê·¸ë¥¼ í–¥í•˜ì—¬!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "820d0eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'I hope you study hard.'},\n",
       " {'translation_text': \"I'd like you to take the class really hard.\"},\n",
       " {'translation_text': \"I'd like you to work hard on your practice.\"},\n",
       " {'translation_text': 'Please listen to the 23rd, please.'}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator([\n",
    "    'ê³µë¶€ë¥¼ ì—´ì‹¬íˆ í•˜ì‹œë©´ ì¢‹ê² ì–´ìš”.',\n",
    "    'ìˆ˜ì—…ì„ ì—´ì‹¬íˆ ë“¤ìœ¼ì‹œë©´ ì¢‹ê² ê³ ìš”.',\n",
    "    'ì‹¤ìŠµë„ ì—´ì‹¬íˆ í•´ì£¼ì‹œë©´ ì¢‹ê² ì–´ìš”.',\n",
    "    'ì œë°œ 23ê¸° ë§ ì¢€... ë“¤ìœ¼ì„¸ìš”...'\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18ea992",
   "metadata": {},
   "source": [
    "##### ê°ì„± ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e2cfbd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "sentiment_clf = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "89d7c532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998722076416016}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_clf(\"I'm very happy!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da33cbfd",
   "metadata": {},
   "source": [
    "- í•œêµ­ì–´\n",
    "\n",
    "https://huggingface.co/sangrimlee/bert-base-multilingual-cased-nsmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d4d084c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "ko_sentiment_clf = pipeline('sentiment-analysis', model='sangrimlee/bert-base-multilingual-cased-nsmc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "706ac9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.9047993421554565},\n",
       " {'label': 'negative', 'score': 0.7013086676597595},\n",
       " {'label': 'positive', 'score': 0.7580994367599487}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_sentiment_clf([\n",
    "    'ì–´ì œ ë°°ìš´ Attentionì´ ì–´ë ¤ì›Œì„œ ë§¥ì£¼ë¥¼ í•œ ì” í–ˆì–´.',\n",
    "    'ê³µë¶€ëŠ” ì–´ë µì§€ë§Œ ë§¥ì£¼ëŠ” ì‹œì›í•˜ë”ë¼',\n",
    "    'í•˜ì§€ë§Œ ë‚˜ëŠ” êµ´í•˜ì§€ ì•Šê³  ì—´ì‹¬íˆ ê³µë¶€í• ê±°ì•¼!!!'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaec0cf",
   "metadata": {},
   "source": [
    "##### Zero Shot Classification\n",
    "\n",
    "- shot == ì˜ˆì‹œ\n",
    "- ì˜ˆì‹œ ì—†ì´ (í•™ìŠµ ì—†ì´) ì¶”ë¡ (ë¶„ë¥˜)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d2b98204",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# ì œë¡œìƒ· ë¶„ë¥˜ íŒŒì´í”„ë¼ì¸(ê¸°ë³¸ NLI ëª¨ë¸ ìë™ ë¡œë“œ)\n",
    "zero_shot_clf = pipeline('zero-shot-classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3f843b47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the transformers library.',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.9053581953048706, 0.07259626686573029, 0.022045595571398735]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_clf(\n",
    "    \"This is a course about the transformers library.\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5f7a260b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': \"It has a poison and it's very dangerous\",\n",
       " 'labels': ['snake', 'tiger', 'squirrel'],\n",
       " 'scores': [0.6373922228813171, 0.19870398938655853, 0.16390381753444672]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_shot_clf(\n",
    "    \"It has a poison and it's very dangerous\",\n",
    "    candidate_labels=[\"tiger\", \"squirrel\", \"snake\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f55e07",
   "metadata": {},
   "source": [
    "- í•œêµ­ì–´\n",
    "\n",
    "https://huggingface.co/joeddav/xlm-roberta-large-xnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1bf790bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "ko_zero_shot_clf = pipeline('zero-shot-classification', model='joeddav/xlm-roberta-large-xnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "16e03551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': '2025ë…„ì—ëŠ” ì–´ë–¤ ìš´ë™ì„ í•˜ì‹œê² ìŠµë‹ˆê¹Œ?!',\n",
       " 'labels': ['ì •ì¹˜', 'ê±´ê°•', 'ê²½ì œ'],\n",
       " 'scores': [0.40395545959472656, 0.3669034242630005, 0.22914111614227295]}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_to_classify = \"2025ë…„ì—ëŠ” ì–´ë–¤ ìš´ë™ì„ í•˜ì‹œê² ìŠµë‹ˆê¹Œ?!\"\n",
    "candidate_labels = [\"ì •ì¹˜\", \"ê²½ì œ\", \"ê±´ê°•\"]\n",
    "\n",
    "ko_zero_shot_clf(sequence_to_classify, candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5676607a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': '2025ë…„ì—ëŠ” ì–´ë–¤ ìš´ë™ì„ í•˜ì‹œê² ìŠµë‹ˆê¹Œ?!',\n",
       " 'labels': ['ê±´ê°•', 'ì •ì¹˜', 'ê²½ì œ'],\n",
       " 'scores': [0.9117788672447205, 0.05924064293503761, 0.02898053266108036]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_to_classify = \"2025ë…„ì—ëŠ” ì–´ë–¤ ìš´ë™ì„ í•˜ì‹œê² ìŠµë‹ˆê¹Œ?!\"\n",
    "candidate_labels = [\"ì •ì¹˜\", \"ê²½ì œ\", \"ê±´ê°•\"]\n",
    "hypothesis_template = \"ì´ í…ìŠ¤íŠ¸ëŠ” {}ì— ê´€í•œ ë‚´ìš©ì…ë‹ˆë‹¤.\"   # NLI ê°€ì„¤ ë¬¸ì¥ í…œí”Œë¦¿\n",
    "\n",
    "ko_zero_shot_clf(sequence_to_classify,                  # ë¬¸ì¥\n",
    "                candidate_labels,                       # í›„ë³´    \n",
    "                hypothesis_template=hypothesis_template # ì‚¬ìš©ì ì •ì˜ ê°€ì„¤ ë¬¸ì¥\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03e06a3",
   "metadata": {},
   "source": [
    "##### í…ìŠ¤íŠ¸ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2ccf514a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "text_generator = pipeline('text-generation', model='distilgpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379f9d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=30) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to do this on the web. We will also teach you how to write a beautiful, beautiful, practical language. These are all new concepts in the course, so please check them out.'},\n",
       " {'generated_text': 'In this course, we will teach you how to get started on your own time and on your own time.'}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generator(\n",
    "    \"In this course, we will teach you how to\", # í…ìŠ¤íŠ¸ ìƒì„± ì‹œì‘ í”„ë¡¬í”„íŠ¸\n",
    "    max_length=30,                              # ìƒì„± ê²°ê³¼ ìµœëŒ€ í† í° ê¸¸ì´\n",
    "    num_return_sequences=2                      # ì„œë¡œ ë‹¤ë¥¸ ìƒì„± ê²°ê³¼ 2ê°œ ë°˜í™˜\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d4fe4d",
   "metadata": {},
   "source": [
    "- í•œêµ­ì–´\n",
    "\n",
    "https://huggingface.co/skt/ko-gpt-trinity-1.2B-v0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fbae860c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "ko_text_generator = pipeline('text-generation', model='skt/ko-gpt-trinity-1.2B-v0.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a631f1f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'ì˜¤ëŠ˜ ì ì‹¬ì—ëŠ” ë­ ë¨¹ì—ˆì–´\\n ë‹µë³€:ê·¸ëŸ¼ìš”. ì‹ì‚¬ ì˜ í•˜ì…¨ì–´ìš”? ìŒì‹ì€ ë˜ë„ë¡ ì‹±ê²ê²Œ ë¨¹ëŠ”ê²Œ ê±´ê°•ì— ì¢‹ëŒ€ìš”.'}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ko_text_generator('AIë€ ë§ì…ë‹ˆë‹¤')\n",
    "ko_text_generator('ì˜¤ëŠ˜ ì ì‹¬ì—ëŠ”')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77abda36",
   "metadata": {},
   "source": [
    "##### Fill Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9dcb64f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "unmasker = pipeline('fill-mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9248f82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.09073042869567871,\n",
       "  'token': 265,\n",
       "  'token_str': ' business',\n",
       "  'sequence': 'This course will teach you all about business model.'},\n",
       " {'score': 0.072625532746315,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical',\n",
       "  'sequence': 'This course will teach you all about mathematical model.'},\n",
       " {'score': 0.049304164946079254,\n",
       "  'token': 5,\n",
       "  'token_str': ' the',\n",
       "  'sequence': 'This course will teach you all about the model.'}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker('This course will teach you all about <mask> model.', top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a902fc24",
   "metadata": {},
   "source": [
    "##### NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "49e2a40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "ner = pipeline('ner', model='dslim/bert-base-NER', grouped_entities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "10a42578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': np.float32(0.98995215),\n",
       "  'word': 'S',\n",
       "  'start': 11,\n",
       "  'end': 12},\n",
       " {'entity_group': 'PER',\n",
       "  'score': np.float32(0.46767735),\n",
       "  'word': '##qui',\n",
       "  'start': 12,\n",
       "  'end': 15},\n",
       " {'entity_group': 'PER',\n",
       "  'score': np.float32(0.72662497),\n",
       "  'word': '##rrel',\n",
       "  'start': 15,\n",
       "  'end': 19},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': np.float32(0.986338),\n",
       "  'word': 'Playdata',\n",
       "  'start': 34,\n",
       "  'end': 42},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': np.float32(0.9994649),\n",
       "  'word': 'Seoul',\n",
       "  'start': 46,\n",
       "  'end': 51}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner('My name is Squirrel and I work at Playdata in Seoul')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b999ca1d",
   "metadata": {},
   "source": [
    "##### Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3c38e7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "qna = pipeline('question-answering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "35bd22e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.8710189713747241, 'start': 34, 'end': 42, 'answer': 'Playdata'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Squirrel and I work at Playdata in Seoul\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39969303",
   "metadata": {},
   "source": [
    "- í•œêµ­ì–´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "355d35b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "ko_qna = pipeline('question-answering', model='klue/roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3406dc85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0002860031963791698, 'start': 138, 'end': 140, 'answer': 'í˜€ë¥¼'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_qna(\n",
    "    question=\"1931ë…„ ì–´ë–¤ ì§„ë‹¨ì„ ë°›ì•˜ë‚˜ìš”?\",\n",
    "    context=\"\"\"ê·¸ëŸ¬ë˜ 1931ë…„, ê·¸ëŠ” ê°‘ì‘ìŠ¤ëŸ½ê²Œ íê²°í•µì„ ì§„ë‹¨ë°›ì•˜ë‹¤. ì´ìƒì€ í•˜ë£¨ì— ë‹´ë°°ë¥¼ 50ê°œí”¼ ì´ìƒ í”¼ëŠ” ê²ƒì„ ìì‹ ì˜ ì¼ê³¼ë¼ê³  í‘œí˜„í–ˆì„ ì •ë„ë¡œ ì—„ì²­ë‚œ ê³¨ì´ˆì˜€ëŠ”ë°, ê·¸ ë•Œë¬¸ì¸ì§€ ë³‘ì„¸ëŠ” ë‚ ì´ ê°ˆìˆ˜ë¡ ì•…í™”ë˜ì–´ ë‹´ë‹¹ì˜ê°€ ê·¸ì˜ íë¥¼ í™•ì¸í•˜ê³ ëŠ” í˜•ì²´ë„ ì•ˆ ë³´ì¸ë‹¤ë©° í˜€ë¥¼ ë‚´ë‘˜ë €ì„ ì •ë„ì˜€ë‹¤. ê²°êµ­ 1933ë…„ë¶€í„°ëŠ” ê°í˜ˆê¹Œì§€ ì‹œì‘ë˜ì—ˆê³ , ê±´ì¶•ê¸°ì‚¬ ì¼ì„ ì§€ì†í•˜ê¸° ì–´ë µë‹¤ê³  íŒë‹¨í•œ ì´ìƒì€ ì¡°ì„ ì´ë…ë¶€ì—ì„œ í‡´ì‚¬í•˜ê³  í™©í•´ë„ì— ìˆëŠ” ë°°ì²œ ì˜¨ì²œìœ¼ë¡œ ìš”ì–‘ì„ ê°„ë‹¤.\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "31c19adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on QuestionAnsweringPipeline in module transformers.pipelines.question_answering object:\n",
      "\n",
      "class QuestionAnsweringPipeline(transformers.pipelines.base.ChunkPipeline)\n",
      " |  QuestionAnsweringPipeline(model: Union[ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')], tokenizer: transformers.tokenization_utils.PreTrainedTokenizer, modelcard: Optional[transformers.modelcard.ModelCard] = None, framework: Optional[str] = None, task: str = '', **kwargs)\n",
      " |\n",
      " |  Question Answering pipeline using any `ModelForQuestionAnswering`. See the [question answering\n",
      " |  examples](../task_summary#question-answering) for more information.\n",
      " |\n",
      " |  Example:\n",
      " |\n",
      " |  ```python\n",
      " |  >>> from transformers import pipeline\n",
      " |\n",
      " |  >>> oracle = pipeline(model=\"deepset/roberta-base-squad2\")\n",
      " |  >>> oracle(question=\"Where do I live?\", context=\"My name is Wolfgang and I live in Berlin\")\n",
      " |  {'score': 0.9191, 'start': 34, 'end': 40, 'answer': 'Berlin'}\n",
      " |  ```\n",
      " |\n",
      " |  Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)\n",
      " |\n",
      " |  This question answering pipeline can currently be loaded from [`pipeline`] using the following task identifier:\n",
      " |  `\"question-answering\"`.\n",
      " |\n",
      " |  The models that this pipeline can use are models that have been fine-tuned on a question answering task. See the\n",
      " |  up-to-date list of available models on\n",
      " |  [huggingface.co/models](https://huggingface.co/models?filter=question-answering).\n",
      " |\n",
      " |  Arguments:\n",
      " |      model ([`PreTrainedModel`] or [`TFPreTrainedModel`]):\n",
      " |          The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from\n",
      " |          [`PreTrainedModel`] for PyTorch and [`TFPreTrainedModel`] for TensorFlow.\n",
      " |      tokenizer ([`PreTrainedTokenizer`]):\n",
      " |          The tokenizer that will be used by the pipeline to encode data for the model. This object inherits from\n",
      " |          [`PreTrainedTokenizer`].\n",
      " |      modelcard (`str` or [`ModelCard`], *optional*):\n",
      " |          Model card attributed to the model for this pipeline.\n",
      " |      framework (`str`, *optional*):\n",
      " |          The framework to use, either `\"pt\"` for PyTorch or `\"tf\"` for TensorFlow. The specified framework must be\n",
      " |          installed.\n",
      " |\n",
      " |          If no framework is specified, will default to the one currently installed. If no framework is specified and\n",
      " |          both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is\n",
      " |          provided.\n",
      " |      task (`str`, defaults to `\"\"`):\n",
      " |          A task-identifier for the pipeline.\n",
      " |      num_workers (`int`, *optional*, defaults to 8):\n",
      " |          When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number of\n",
      " |          workers to be used.\n",
      " |      batch_size (`int`, *optional*, defaults to 1):\n",
      " |          When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of\n",
      " |          the batch to use, for inference this is not always beneficial, please read [Batching with\n",
      " |          pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching) .\n",
      " |      args_parser ([`~pipelines.ArgumentHandler`], *optional*):\n",
      " |          Reference to the object in charge of parsing supplied pipeline parameters.\n",
      " |      device (`int`, *optional*, defaults to -1):\n",
      " |          Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on\n",
      " |          the associated CUDA device id. You can pass native `torch.device` or a `str` too\n",
      " |      dtype (`str` or `torch.dtype`, *optional*):\n",
      " |          Sent directly as `model_kwargs` (just a simpler shortcut) to use the available precision for this model\n",
      " |          (`torch.float16`, `torch.bfloat16`, ... or `\"auto\"`)\n",
      " |      binary_output (`bool`, *optional*, defaults to `False`):\n",
      " |          Flag indicating if the output the pipeline should happen in a serialized format (i.e., pickle) or as\n",
      " |          the raw output data e.g. text.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      QuestionAnsweringPipeline\n",
      " |      transformers.pipelines.base.ChunkPipeline\n",
      " |      transformers.pipelines.base.Pipeline\n",
      " |      transformers.pipelines.base._ScikitCompat\n",
      " |      abc.ABC\n",
      " |      transformers.utils.hub.PushToHubMixin\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Answer the question(s) given as inputs by using the context(s).\n",
      " |\n",
      " |      Args:\n",
      " |          question (`str` or `list[str]`):\n",
      " |              One or several question(s) (must be used in conjunction with the `context` argument).\n",
      " |          context (`str` or `list[str]`):\n",
      " |              One or several context(s) associated with the question(s) (must be used in conjunction with the\n",
      " |              `question` argument).\n",
      " |          top_k (`int`, *optional*, defaults to 1):\n",
      " |              The number of answers to return (will be chosen by order of likelihood). Note that we return less than\n",
      " |              top_k answers if there are not enough options available within the context.\n",
      " |          doc_stride (`int`, *optional*, defaults to 128):\n",
      " |              If the context is too long to fit with the question for the model, it will be split in several chunks\n",
      " |              with some overlap. This argument controls the size of that overlap.\n",
      " |          max_answer_len (`int`, *optional*, defaults to 15):\n",
      " |              The maximum length of predicted answers (e.g., only answers with a shorter length are considered).\n",
      " |          max_seq_len (`int`, *optional*, defaults to 384):\n",
      " |              The maximum length of the total sentence (context + question) in tokens of each chunk passed to the\n",
      " |              model. The context will be split in several chunks (using `doc_stride` as overlap) if needed.\n",
      " |          max_question_len (`int`, *optional*, defaults to 64):\n",
      " |              The maximum length of the question after tokenization. It will be truncated if needed.\n",
      " |          handle_impossible_answer (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not we accept impossible as an answer.\n",
      " |          align_to_words (`bool`, *optional*, defaults to `True`):\n",
      " |              Attempts to align the answer to real words. Improves quality on space separated languages. Might hurt on\n",
      " |              non-space-separated languages (like Japanese or Chinese)\n",
      " |\n",
      " |      Return:\n",
      " |          A `dict` or a list of `dict`: Each result comes as a dictionary with the following keys:\n",
      " |\n",
      " |          - **score** (`float`) -- The probability associated to the answer.\n",
      " |          - **start** (`int`) -- The character start index of the answer (in the tokenized version of the input).\n",
      " |          - **end** (`int`) -- The character end index of the answer (in the tokenized version of the input).\n",
      " |          - **answer** (`str`) -- The answer to the question.\n",
      " |\n",
      " |  __init__(self, model: Union[ForwardRef('PreTrainedModel'), ForwardRef('TFPreTrainedModel')], tokenizer: transformers.tokenization_utils.PreTrainedTokenizer, modelcard: Optional[transformers.modelcard.ModelCard] = None, framework: Optional[str] = None, task: str = '', **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  get_answer(self, answers: list[dict], target: str) -> Optional[dict]\n",
      " |\n",
      " |  get_indices(self, enc: 'tokenizers.Encoding', s: int, e: int, sequence_index: int, align_to_words: bool) -> tuple[int, int]\n",
      " |\n",
      " |  postprocess(self, model_outputs, top_k=1, handle_impossible_answer=False, max_answer_len=15, align_to_words=True)\n",
      " |      Postprocess will receive the raw outputs of the `_forward` method, generally tensors, and reformat them into\n",
      " |      something more friendly. Generally it will output a list or a dict or results (containing just strings and\n",
      " |      numbers).\n",
      " |\n",
      " |  preprocess(self, example, padding='do_not_pad', doc_stride=None, max_question_len=64, max_seq_len=None)\n",
      " |      Preprocess will take the `input_` of a specific pipeline and return a dictionary of everything necessary for\n",
      " |      `_forward` to run properly. It should contain at least one tensor, but might have arbitrary other items.\n",
      " |\n",
      " |  span_to_answer(self, text: str, start: int, end: int) -> dict[str, typing.Union[str, int]]\n",
      " |      When decoding from token probabilities, this method maps token indexes to actual word in the initial context.\n",
      " |\n",
      " |      Args:\n",
      " |          text (`str`): The actual context to extract the answer from.\n",
      " |          start (`int`): The answer starting token index.\n",
      " |          end (`int`): The answer end token index.\n",
      " |\n",
      " |      Returns:\n",
      " |          Dictionary like `{'answer': str, 'start': int, 'end': int}`\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |\n",
      " |  create_sample(question: Union[str, list[str]], context: Union[str, list[str]]) -> Union[transformers.data.processors.squad.SquadExample, list[transformers.data.processors.squad.SquadExample]]\n",
      " |      QuestionAnsweringPipeline leverages the [`SquadExample`] internally. This helper method encapsulate all the\n",
      " |      logic for converting question(s) and context(s) to [`SquadExample`].\n",
      " |\n",
      " |      We currently support extractive question answering.\n",
      " |\n",
      " |      Arguments:\n",
      " |          question (`str` or `list[str]`): The question(s) asked.\n",
      " |          context (`str` or `list[str]`): The context(s) in which we will look for the answer.\n",
      " |\n",
      " |      Returns:\n",
      " |          One or a list of [`SquadExample`]: The corresponding [`SquadExample`] grouping question and context.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __abstractmethods__ = frozenset()\n",
      " |\n",
      " |  default_input_names = 'question,context'\n",
      " |\n",
      " |  handle_impossible_answer = False\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.pipelines.base.ChunkPipeline:\n",
      " |\n",
      " |  get_iterator(self, inputs, num_workers: int, batch_size: int, preprocess_params, forward_params, postprocess_params)\n",
      " |\n",
      " |  run_single(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.pipelines.base.Pipeline:\n",
      " |\n",
      " |  check_model_type(self, supported_models: Union[list[str], dict])\n",
      " |      Check if the model class is in supported by the pipeline.\n",
      " |\n",
      " |      Args:\n",
      " |          supported_models (`list[str]` or `dict`):\n",
      " |              The list of models supported by the pipeline, or a dictionary with model class values.\n",
      " |\n",
      " |  device_placement(self)\n",
      " |      Context Manager allowing tensor allocation on the user-specified device in framework agnostic way.\n",
      " |\n",
      " |      Returns:\n",
      " |          Context manager\n",
      " |\n",
      " |      Examples:\n",
      " |\n",
      " |      ```python\n",
      " |      # Explicitly ask for tensor allocation on CUDA device :0\n",
      " |      pipe = pipeline(..., device=0)\n",
      " |      with pipe.device_placement():\n",
      " |          # Every framework specific tensor allocation will be done on the request device\n",
      " |          output = pipe(...)\n",
      " |      ```\n",
      " |\n",
      " |  ensure_tensor_on_device(self, **inputs)\n",
      " |      Ensure PyTorch tensors are on the specified device.\n",
      " |\n",
      " |      Args:\n",
      " |          inputs (keyword arguments that should be `torch.Tensor`, the rest is ignored):\n",
      " |              The tensors to place on `self.device`.\n",
      " |          Recursive on lists **only**.\n",
      " |\n",
      " |      Return:\n",
      " |          `dict[str, torch.Tensor]`: The same as `inputs` but on the proper device.\n",
      " |\n",
      " |  forward(self, model_inputs, **forward_params)\n",
      " |\n",
      " |  get_inference_context(self)\n",
      " |\n",
      " |  iterate(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |\n",
      " |  predict(self, X)\n",
      " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
      " |\n",
      " |  push_to_hub(self, repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Union[bool, str, NoneType] = None, max_shard_size: Union[str, int, NoneType] = '5GB', create_pr: bool = False, safe_serialization: bool = True, revision: Optional[str] = None, commit_description: Optional[str] = None, tags: Optional[list[str]] = None, **deprecated_kwargs) -> str from transformers.utils.hub.PushToHubMixin\n",
      " |      Upload the pipeline file to the ğŸ¤— Model Hub.\n",
      " |\n",
      " |      Parameters:\n",
      " |          repo_id (`str`):\n",
      " |              The name of the repository you want to push your pipe to. It should contain your organization name\n",
      " |              when pushing to a given organization.\n",
      " |          use_temp_dir (`bool`, *optional*):\n",
      " |              Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub.\n",
      " |              Will default to `True` if there is no directory named like `repo_id`, `False` otherwise.\n",
      " |          commit_message (`str`, *optional*):\n",
      " |              Message to commit while pushing. Will default to `\"Upload pipe\"`.\n",
      " |          private (`bool`, *optional*):\n",
      " |              Whether to make the repo private. If `None` (default), the repo will be public unless the organization's default is private. This value is ignored if the repo already exists.\n",
      " |          token (`bool` or `str`, *optional*):\n",
      " |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
      " |              when running `hf auth login` (stored in `~/.huggingface`). Will default to `True` if `repo_url`\n",
      " |              is not specified.\n",
      " |          max_shard_size (`int` or `str`, *optional*, defaults to `\"5GB\"`):\n",
      " |              Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard\n",
      " |              will then be each of size lower than this size. If expressed as a string, needs to be digits followed\n",
      " |              by a unit (like `\"5MB\"`). We default it to `\"5GB\"` so that users can easily load models on free-tier\n",
      " |              Google Colab instances without any CPU OOM issues.\n",
      " |          create_pr (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether or not to create a PR with the uploaded files or directly commit.\n",
      " |          safe_serialization (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether or not to convert the model weights in safetensors format for safer serialization.\n",
      " |          revision (`str`, *optional*):\n",
      " |              Branch to push the uploaded files to.\n",
      " |          commit_description (`str`, *optional*):\n",
      " |              The description of the commit that will be created\n",
      " |          tags (`list[str]`, *optional*):\n",
      " |              List of tags to push on the Hub.\n",
      " |\n",
      " |      Examples:\n",
      " |\n",
      " |      ```python\n",
      " |      from transformers import pipeline\n",
      " |\n",
      " |      pipe = pipeline(\"google-bert/bert-base-cased\")\n",
      " |\n",
      " |      # Push the pipe to your namespace with the name \"my-finetuned-bert\".\n",
      " |      pipe.push_to_hub(\"my-finetuned-bert\")\n",
      " |\n",
      " |      # Push the pipe to an organization with the name \"my-finetuned-bert\".\n",
      " |      pipe.push_to_hub(\"huggingface/my-finetuned-bert\")\n",
      " |      ```\n",
      " |\n",
      " |  run_multi(self, inputs, preprocess_params, forward_params, postprocess_params)\n",
      " |\n",
      " |  save_pretrained(self, save_directory: Union[str, os.PathLike], safe_serialization: bool = True, **kwargs: Any)\n",
      " |      Save the pipeline's model and tokenizer.\n",
      " |\n",
      " |      Args:\n",
      " |          save_directory (`str` or `os.PathLike`):\n",
      " |              A path to the directory where to saved. It will be created if it doesn't exist.\n",
      " |          safe_serialization (`str`):\n",
      " |              Whether to save the model using `safetensors` or the traditional way for PyTorch or Tensorflow.\n",
      " |          kwargs (`dict[str, Any]`, *optional*):\n",
      " |              Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n",
      " |\n",
      " |  transform(self, X)\n",
      " |      Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from transformers.pipelines.base.Pipeline:\n",
      " |\n",
      " |  dtype\n",
      " |      Dtype of the model (if it's Pytorch model), `None` otherwise.\n",
      " |\n",
      " |  torch_dtype\n",
      " |      Torch dtype of the model (if it's Pytorch model), `None` otherwise.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.pipelines.base._ScikitCompat:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ko_qna)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
