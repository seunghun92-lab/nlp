{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e53662c1",
   "metadata": {},
   "source": [
    "# 토큰화 (Tokenization)\n",
    "\n",
    "- 문장이나 단어를 더 작은 단위로 나누어 분석 가능한 단위(토큰, Token)으로 변환하는 과정\n",
    "- 토큰의 단위가 상황에 따라 다르지만, 보통 의미있는 혹은 처리하는 단위로써 토큰 정의\n",
    "- 자연어 처리에서 크롤링, 데이터 수집 등으로 얻은 코퍼스 데이터는 정제되지 않은 경우가 많은데 이를 사용 용도에 맞게 토큰화, 정제, 정규화하는 과정이 필요\n",
    "\n",
    "**토큰화 목적**\n",
    "\n",
    "- 문법적 구조 이해\n",
    "- 유연한 데이터 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "392bd35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\nlp\\nlp_venv\\lib\\site-packages (4.57.6)\n",
      "Requirement already satisfied: spacy in c:\\nlp\\nlp_venv\\lib\\site-packages (3.8.11)\n",
      "Requirement already satisfied: kss==5.0.0 in c:\\nlp\\nlp_venv\\lib\\site-packages (5.0.0)\n",
      "Requirement already satisfied: tensorflow in c:\\nlp\\nlp_venv\\lib\\site-packages (2.20.0)\n",
      "Requirement already satisfied: emoji==1.2.0 in c:\\nlp\\nlp_venv\\lib\\site-packages (from kss==5.0.0) (1.2.0)\n",
      "Requirement already satisfied: regex in c:\\nlp\\nlp_venv\\lib\\site-packages (from kss==5.0.0) (2026.1.15)\n",
      "Requirement already satisfied: pecab in c:\\nlp\\nlp_venv\\lib\\site-packages (from kss==5.0.0) (1.0.8)\n",
      "Requirement already satisfied: networkx in c:\\nlp\\nlp_venv\\lib\\site-packages (from kss==5.0.0) (3.6.1)\n",
      "Requirement already satisfied: filelock in c:\\nlp\\nlp_venv\\lib\\site-packages (from transformers) (3.20.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\nlp\\nlp_venv\\lib\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\nlp\\nlp_venv\\lib\\site-packages (from transformers) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\nlp\\nlp_venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\nlp\\nlp_venv\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: requests in c:\\nlp\\nlp_venv\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\nlp\\nlp_venv\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\nlp\\nlp_venv\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\nlp\\nlp_venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\nlp\\nlp_venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2026.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\nlp\\nlp_venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\nlp\\nlp_venv\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\nlp\\nlp_venv\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\nlp\\nlp_venv\\lib\\site-packages (from spacy) (1.0.15)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\nlp\\nlp_venv\\lib\\site-packages (from spacy) (2.0.13)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\nlp\\nlp_venv\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\nlp\\nlp_venv\\lib\\site-packages (from spacy) (8.3.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\nlp\\nlp_venv\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\nlp\\nlp_venv\\lib\\site-packages (from spacy) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\nlp\\nlp_venv\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in c:\\nlp\\nlp_venv\\lib\\site-packages (from spacy) (0.4.3)\n",
      "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in c:\\nlp\\nlp_venv\\lib\\site-packages (from spacy) (0.21.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\nlp\\nlp_venv\\lib\\site-packages (from spacy) (2.12.5)\n",
      "Requirement already satisfied: jinja2 in c:\\nlp\\nlp_venv\\lib\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\nlp\\nlp_venv\\lib\\site-packages (from spacy) (80.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\nlp\\nlp_venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\nlp\\nlp_venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\nlp\\nlp_venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\nlp\\nlp_venv\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\nlp\\nlp_venv\\lib\\site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\nlp\\nlp_venv\\lib\\site-packages (from requests->transformers) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\nlp\\nlp_venv\\lib\\site-packages (from requests->transformers) (2026.1.4)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\nlp\\nlp_venv\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\nlp\\nlp_venv\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\nlp\\nlp_venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\nlp\\nlp_venv\\lib\\site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\nlp\\nlp_venv\\lib\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\nlp\\nlp_venv\\lib\\site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
      "Requirement already satisfied: wrapt in c:\\nlp\\nlp_venv\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow) (25.12.19)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow) (0.7.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow) (6.33.4)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow) (3.13.1)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow) (3.15.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\nlp\\nlp_venv\\lib\\site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\nlp\\nlp_venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: pillow in c:\\nlp\\nlp_venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (12.1.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\nlp\\nlp_venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\nlp\\nlp_venv\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.5)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\nlp\\nlp_venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\nlp\\nlp_venv\\lib\\site-packages (from keras>=3.10.0->tensorflow) (14.2.0)\n",
      "Requirement already satisfied: namex in c:\\nlp\\nlp_venv\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\nlp\\nlp_venv\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.18.0)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in c:\\nlp\\nlp_venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: pyarrow in c:\\nlp\\nlp_venv\\lib\\site-packages (from pecab->kss==5.0.0) (23.0.0)\n",
      "Requirement already satisfied: pytest in c:\\nlp\\nlp_venv\\lib\\site-packages (from pecab->kss==5.0.0) (9.0.2)\n",
      "Requirement already satisfied: iniconfig>=1.0.1 in c:\\nlp\\nlp_venv\\lib\\site-packages (from pytest->pecab->kss==5.0.0) (2.3.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in c:\\nlp\\nlp_venv\\lib\\site-packages (from pytest->pecab->kss==5.0.0) (1.6.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in c:\\nlp\\nlp_venv\\lib\\site-packages (from pytest->pecab->kss==5.0.0) (2.19.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\nlp\\nlp_venv\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\nlp\\nlp_venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers spacy \"kss==5.0.0\" tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c1d4533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.12 버전의 python 가상환경\n",
    "# python3.12 -m venv (가상환경명)\n",
    "# python3.12 -m venv nlp_venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6173cba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')          # 문장/단어 토큰화용 리소스\n",
    "nltk.download('punkt_tab')      # punkt 관련 추가 테이블 리소스\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a4cce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"NLP is fascinating. It has many applications in real-world scenarios.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85e09768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLP',\n",
       " 'is',\n",
       " 'fascinating',\n",
       " '.',\n",
       " 'It',\n",
       " 'has',\n",
       " 'many',\n",
       " 'applications',\n",
       " 'in',\n",
       " 'real-world',\n",
       " 'scenarios',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK 토큰화\n",
    "import nltk\n",
    "\n",
    "nltk.word_tokenize(text)    # text를 단어(토큰) 리스트로 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9075cfe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP is fascinating.', 'It has many applications in real-world scenarios.']\n"
     ]
    }
   ],
   "source": [
    "print(nltk.sent_tokenize(text)) # text를 문장 리스트로 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "574357ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP', 'is', 'fascinating', '.']\n",
      "['It', 'has', 'many', 'applications', 'in', 'real-world', 'scenarios', '.']\n"
     ]
    }
   ],
   "source": [
    "for sent in nltk.sent_tokenize(text):   # 문장 단위로 순회\n",
    "    print(nltk.word_tokenize(sent))     # 각 문장을 단어 토큰으로 분리해 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0269ec70",
   "metadata": {},
   "source": [
    "### Subword Tokenization\n",
    "\n",
    "- BertTokenizer\n",
    "    - 단어를 부분 단위로 쪼개어 희귀하거나, 새로운 단어도 부분적으로 표현 할 수 있도록 한다.  \n",
    "    - 어휘 크기를 줄이고 다양한 언어 패턴 학습 가능  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674ee928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\nlp\\nlp_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['un', '##ha', '##pp', '##iness']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  # 소문자 기준\n",
    "\n",
    "#word = 'happy'\n",
    "word = 'unhappiness'\n",
    "subwords = tokenizer.tokenize(word)\n",
    "subwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e254e2",
   "metadata": {},
   "source": [
    "### - BERT의 subword 토큰화  \n",
    "    - 단어를 사전에 있는 조각(Subword) 단위로 쪼개어 표현  \n",
    "    - 처음 조각 그대로, 단어 중간/뒤의 단어는 ##를 붙여서 표현한다.  \n",
    "    - 통째로 완전한 단어가 없으면 가장 긴 조각을 먼저 찾아 쪼갠다.\n",
    "    - 끝까지 조각이 안맞으면 [UNK] (unknown) 토큰으로 처리한다.  \n",
    "    - 처음 보는 단어를 줄이고, 희귀 단어를 조각 조합으로 찾을 수 있도록 한다. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7101fd8d",
   "metadata": {},
   "source": [
    "### - 문자 단위 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d74a132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u', 'n', 'h', 'a', 'p', 'p', 'i', 'n', 'e', 's', 's']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7843fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Time',\n",
       " 'files',\n",
       " 'like',\n",
       " 'an',\n",
       " 'arrow',\n",
       " 'fruit',\n",
       " 'files',\n",
       " 'like',\n",
       " 'a',\n",
       " 'banana']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정규표현식으로 단어 추출\n",
    "import re\n",
    "\n",
    "text = 'Time files like an arrow; fruit files like a banana'\n",
    "re.findall(r\"\\b\\w+\\b\", text)    # 단어 경계 기준으로 단어(문자/숫자/_)를 전부 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a445b6",
   "metadata": {},
   "source": [
    "\\b : 단어 경계. \\b ... \\b 로 감싸면 안에 있는 패턴 형태만 추출  \n",
    "\\w : 문자/숫자_ 1개 (\\w+ : 문자 덩어리)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a28164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'hesitate', 'to', 'use', 'well', '-', 'being', 'practices', 'for', 'self', '-', 'care', '.']\n",
      "['Do', \"n't\", 'hesitate', 'to', 'use', 'well-being', 'practices', 'for', 'self-care', '.']\n"
     ]
    }
   ],
   "source": [
    "# wordPuncTokenizer(단어/구두점으로 토큰을 구분. ',- 포함 단어도 분리) 와 word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer, word_tokenize\n",
    "\n",
    "text = \"Don't hesitate to use well-being practices for self-care.\"\n",
    "word_punct_tokenizer = WordPunctTokenizer()\n",
    "print(word_punct_tokenizer.tokenize(text))\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9c420e",
   "metadata": {},
   "source": [
    "WordPunctTokenizer 는 ',- 같은 구두점 기준으로 더 잘개 쪼갠다.  \n",
    "word_tokenize 는 상대적으로 자연스러운 단어 단위로 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af3ea7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COVID-19', '(', '전염병', ')', ',', 'Dr.Smith', '(', '의사', ')', ',', 'NASA', '(', '우주항공국', ')', '등', '특정', '기관이나', '명칭이', '있다.', '특수', '문자', '또한', '태그', '<', 'br', '>', ',', '가격', '$', '100.50', ',', '2025/08/18', '날짜', '표현에', '사용될', '수', '있다.', '이러한', '경우', ',', '$', '100.50을', '하나의', '토큰으로', '유지할', '필요가', '있다', '.']\n",
      "['COVID-19', '(', '전염병', ')', ',', 'Dr.Smith', '(', '의사', ')', ',', 'NASA', '(', '우주항공국', ')', '등', '특정', '기관이나', '명칭이', '있다', '.', '특수', '문자', '또한', '태그', '<', 'br', '>', ',', '가격', '$', '100.50', ',', '2025/08/18', '날짜', '표현에', '사용될', '수', '있다', '.', '이러한', '경우', ',', '$', '100.50을', '하나의', '토큰으로', '유지할', '필요가', '있다', '.']\n"
     ]
    }
   ],
   "source": [
    "# TreebankWordTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer, word_tokenize\n",
    "\n",
    "text = '''\n",
    "COVID-19(전염병), Dr.Smith(의사), NASA(우주항공국) 등 특정 기관이나 명칭이 있다.\n",
    "특수 문자 또한 태그 <br>, 가격 $100.50, 2025/08/18 날짜 표현에 사용될 수 있다.\n",
    "이러한 경우, $100.50을 하나의 토큰으로 유지할 필요가 있다.\n",
    "'''\n",
    "\n",
    "treebank_word_tokenizer = TreebankWordTokenizer()\n",
    "print(treebank_word_tokenizer.tokenize(text))                   # Treebank 규칙 기반 토크나이저\n",
    "print(word_tokenize(text))                                      # NLTK 기본 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c8dbf4",
   "metadata": {},
   "source": [
    "하이픈, 괄호, <br>, $100.50, 날짜 등이 있을때 추가적으로 차이가 발생할 수 있다.  \n",
    "현재 예문에서는 있다. 정도 차이  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d4d105",
   "metadata": {},
   "source": [
    "### 한국어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f9ee934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Kss]: Because there's no supported C++ morpheme analyzer, Kss will take pecab as a backend. :D\n",
      "For your information, Kss also supports mecab backend.\n",
      "We recommend you to install mecab or konlpy.tag.Mecab for faster execution of Kss.\n",
      "Please refer to following web sites for details:\n",
      "- mecab: https://cleancode-ws.tistory.com/97\n",
      "- konlpy.tag.Mecab: https://uwgdqo.tistory.com/363\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['시간적 배경은 1920년대의 겨울로, 공간적 배경은 경성부.',\n",
       " '주인공이자 인력거꾼 김 첨지의 아내는 병에 걸린 지 1달 가량이 지나 있었다.',\n",
       " \"아내는 단 한 번도 약을 먹어본 적이 없는데, 그 이유는 '병이란 놈에게 약을 주어 보내면 재미를 붙여서 자꾸 온다'는 김 첨지의 신조 때문으로 나오지만 사실 이건 핑계고, 약을 살 돈도 벌지 못하고 있었다\",\n",
       " '는 이유가 더 크다.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kss  #한국어 문장 분리 라이브러리(Korean Sentence Splitter)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "text = \"시간적 배경은 1920년대의 겨울로, 공간적 배경은 경성부. 주인공이자 인력거꾼 김 첨지의 아내는 병에 걸린 지 1달 가량이 지나 있었다. 아내는 단 한 번도 약을 먹어본 적이 없는데, 그 이유는 '병이란 놈에게 약을 주어 보내면 재미를 붙여서 자꾸 온다'는 김 첨지의 신조 때문으로 나오지만 사실 이건 핑계고, 약을 살 돈도 벌지 못하고 있었다는 이유가 더 크다.\"\n",
    "\n",
    "kss.split_sentences(text)   # 문장 단위로 분리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33459cf",
   "metadata": {},
   "source": [
    "### 품사 태깅\n",
    "\n",
    "**pos_tag**\n",
    "\n",
    "pos_tag는 자연어 처리(NLP)에서 단어에 품사를 태깅하는 함수로, 주로 NLTK와 같은 라이브러리에서 사용된다.\n",
    "\n",
    "- nltk pos_tag() 주요 품사 태깅<br>\n",
    "\n",
    "1. **NN (Noun, Singular)**  \n",
    "   단수 명사를 나타낸다. 하나의 사물이나 개념을 지칭한다.  \n",
    "   예시: \"cat\", \"book\", \"apple\"\n",
    "\n",
    "2. **NNS (Noun, Plural)**  \n",
    "   복수 명사를 나타낸다. 두 개 이상의 사물이나 개념을 지칭한다.  \n",
    "   예시: \"cats\", \"books\", \"apples\"\n",
    "\n",
    "3. **NNP (Proper Noun, Singular)**  \n",
    "   단수 고유 명사를 나타낸다. 특정한 사람, 장소 또는 조직의 이름을 지칭한다.  \n",
    "   예시: \"Alice\", \"London\", \"NASA\"\n",
    "\n",
    "4. **NNPS (Proper Noun, Plural)**  \n",
    "   복수 고유 명사를 나타낸다. 두 개 이상의 특정한 사람, 장소 또는 조직의 이름을 지칭한다.  \n",
    "   예시: \"Smiths\", \"United Nations\"\n",
    "\n",
    "5. **VB (Verb, Base Form)**  \n",
    "   동사의 원형을 나타낸다. 일반적으로 현재 시제와 함께 사용된다.  \n",
    "   예시: \"run\", \"eat\", \"play\"\n",
    "\n",
    "6. **VBD (Verb, Past Tense)**  \n",
    "   동사의 과거형을 나타낸다.  \n",
    "   예시: \"ran\", \"ate\", \"played\"\n",
    "\n",
    "7. **VBG (Verb, Gerund or Present Participle)**  \n",
    "   동명사 또는 현재 분사를 나타낸다. 일반적으로 \"-ing\" 형태이다.  \n",
    "   예시: \"running\", \"eating\", \"playing\"\n",
    "\n",
    "8. **VBN (Verb, Past Participle)**  \n",
    "   동사의 과거 분사형을 나타낸다. 주로 완료 시제와 함께 사용된다.  \n",
    "   예시: \"run\" (as in \"has run\"), \"eaten\", \"played\"\n",
    "\n",
    "9. **VBZ (Verb, 3rd Person Singular Present)**  \n",
    "   3인칭 단수 현재형 동사를 나타낸다. 주어가 3인칭 단수일 때 사용된다.  \n",
    "   예시: \"runs\", \"eats\", \"plays\"\n",
    "\n",
    "10. **JJ (Adjective)**  \n",
    "    형용사를 나타낸다. 명사를 수식하여 그 특성을 설명한다.  \n",
    "    예시: \"big\", \"blue\", \"happy\"\n",
    "\n",
    "11. **JJR (Adjective, Comparative)**  \n",
    "    비교급 형용사를 나타낸다. 두 개의 대상을 비교할 때 사용된다.  \n",
    "    예시: \"bigger\", \"bluer\", \"happier\"\n",
    "\n",
    "12. **JJS (Adjective, Superlative)**  \n",
    "    최상급 형용사를 나타낸다. 세 개 이상의 대상을 비교할 때 사용된다.  \n",
    "    예시: \"biggest\", \"bluest\", \"happiest\"\n",
    "\n",
    "13. **RB (Adverb)**  \n",
    "    부사를 나타낸다. 동사, 형용사 또는 다른 부사를 수식한다.  \n",
    "    예시: \"quickly\", \"very\", \"well\"\n",
    "\n",
    "14. **RBR (Adverb, Comparative)**  \n",
    "    비교급 부사를 나타낸다. 두 개의 대상을 비교할 때 사용된다.  \n",
    "    예시: \"more quickly\", \"better\"\n",
    "\n",
    "15. **RBS (Adverb, Superlative)**  \n",
    "    최상급 부사를 나타낸다. 세 개 이상의 대상을 비교할 때 사용된다.  \n",
    "    예시: \"most quickly\", \"best\"\n",
    "\n",
    "16. **IN (Preposition or Subordinating Conjunction)**  \n",
    "    전치사 또는 종속 접속사를 나타낸다. 명사와의 관계를 나타내거나 종속절을 시작한다.  \n",
    "    예시: \"in\", \"on\", \"because\"\n",
    "\n",
    "17. **DT (Determiner)**  \n",
    "    한정사를 나타낸다. 명사의 수와 상태를 정의한다.  \n",
    "    예시: \"the\", \"a\", \"some\"\n",
    "\n",
    "18. **PRP (Personal Pronoun)**  \n",
    "    인칭 대명사를 나타낸다. 사람, 사물 등을 대체할 때 사용된다.  \n",
    "    예시: \"I\", \"you\", \"he\", \"they\"\n",
    "\n",
    "19. **PRP$ (Possessive Pronoun)**  \n",
    "    소유 대명사를 나타낸다. 소유 관계를 나타낸다.  \n",
    "    예시: \"my\", \"your\", \"his\", \"their\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "519e709e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Playdata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK 품사 태그 리소스 다운로드\n",
    "nltk.download('averaged_perceptron_tagger')     # NLTK 기본 POS TAGGER 모델\n",
    "nltk.download('averaged_perceptron_tagger_eng') # 영어 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f34ef51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Time', 'NNP'),\n",
       " ('flies', 'NNS'),\n",
       " ('like', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('arrow', 'NN')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "\n",
    "text = 'Time flies like an arrow'   \n",
    "tokens = word_tokenize(text)        # 단어 토큰화\n",
    "pos_tags = pos_tag(tokens)          # 각 토큰에 품사 태그 부여 : (토큰,품사태그) 형태의 리스트\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca29090",
   "metadata": {},
   "source": [
    "**spacy 주요 품사 태깅**\n",
    "\n",
    "| 태그 | 설명                 | 예시                 |\n",
    "|------|----------------------|----------------------|\n",
    "| ADJ  | 형용사               | big, nice           |\n",
    "| ADP  | 전치사               | in, to, on          |\n",
    "| ADV  | 부사                 | very, well          |\n",
    "| AUX  | 조동사               | is, have (조동사로 사용될 때) |\n",
    "| CONJ | 접속사               | and, or             |\n",
    "| DET  | 한정사/관사          | the, a              |\n",
    "| INTJ | 감탄사               | oh, wow             |\n",
    "| NOUN | 명사                 | dog, table          |\n",
    "| NUM  | 숫자                 | one, two, 3         |\n",
    "| PART | 소사                 | 'to' (to fly에서), not |\n",
    "| PRON | 대명사               | he, she, it         |\n",
    "| PROPN| 고유명사             | John, France        |\n",
    "| PUNCT| 구두점               | ., !, ?             |\n",
    "| SCONJ| 종속 접속사          | because, if         |\n",
    "| SYM  | 기호                 | $, %, @             |\n",
    "| VERB | 동사                 | run, eat            |\n",
    "| X    | 알 수 없는 품사       | 외국어 단어, 잘못된 형식 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f156f76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "\n",
    "spacy.cli.download('en_core_web_sm')        # spacy 영어 모델 다운로드\n",
    "spacy.nlp = spacy.load('en_core_web_sm')    # 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84b2f3d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time flies like an arrow"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# 영어 모델 로드\n",
    "spacy_nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = 'Time flies like an arrow'\n",
    "tokens = spacy_nlp(text)\n",
    "\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "256a99b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time : NOUN\n",
      "flies : VERB\n",
      "like : ADP\n",
      "an : DET\n",
      "arrow : NOUN\n"
     ]
    }
   ],
   "source": [
    "for token in tokens:\n",
    "    print(token.text, ':', token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd30530f",
   "metadata": {},
   "source": [
    "### KoNLPy\n",
    "\n",
    "- 한국어 자연어 처리를 위한 라이브러리\n",
    "- 형태소 분석, 품사 태깅, 텍스트 전처리 등 기능 지원\n",
    "- 여러 형태소 분석기 중 적합한 분석기 선택 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c523f75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘', '점심', '은', '뭐', '먹을까', '?', '맛있는', '게', '뭐', '지', '?']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Okt 형태소 분석\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "text = '오늘 점심은 뭐 먹을까? 맛있는 게 뭐지?'\n",
    "okt = Okt(jvmpath=r\"C:\\Program Files\\Java\\jdk-21\\bin\\server\\jvm.dll\")\n",
    "\n",
    "\n",
    "morphs = okt.morphs(text)   # 형태소 단위로 분리\n",
    "morphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d562959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('오늘', 'Noun'),\n",
       " ('점심', 'Noun'),\n",
       " ('은', 'Josa'),\n",
       " ('뭐', 'Noun'),\n",
       " ('먹을까', 'Verb'),\n",
       " ('?', 'Punctuation'),\n",
       " ('맛있는', 'Adjective'),\n",
       " ('게', 'Noun'),\n",
       " ('뭐', 'Noun'),\n",
       " ('지', 'Josa'),\n",
       " ('?', 'Punctuation')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tags = okt.pos(text)\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89c8847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘', '점심', '뭐', '게', '뭐']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 명사 추출\n",
    "nouns = okt.nouns(text) # 문장에서 명사만 추출\n",
    "nouns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
