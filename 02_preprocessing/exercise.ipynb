{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b93473ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "처리된 텍스트: hello 자연어 처리 is fun do you agree\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 원시 텍스트\n",
    "raw_text = \"Hello!!! 자연어 처리 is FUN.   Do you agree? :)\"\n",
    "\n",
    "# 불필요한 기호 제거 : \\w(문자/숫자/_), \\s(공백)만 남기 나머지 특수문자는 제거\n",
    "clean_text = re.sub(r\"[^\\w\\s]\", \"\", raw_text)\n",
    "\n",
    "# 소문자 변환: 영어 대문자를 소문자로 통일\n",
    "clean_text = clean_text.lower()\n",
    "\n",
    "# 공백 제거 : 연속 공백/앞뒤 공백 제거 후 단어 사이클 공백 1칸으로 정리\n",
    "clean_text = \" \".join(clean_text.split())\n",
    "\n",
    "print(\"처리된 텍스트:\", clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bef31e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['자연어 처리는 재미있다!', '하지만 배우기는 어렵다.', 'Python으로 가능합니다.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"자연어 처리는 재미있다! 하지만 배우기는 어렵다. Python으로 가능합니다.\"\n",
    "print(sent_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1bc07df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords                   # 불용어 목록\n",
    "from nltk.tokenize import word_tokenize             # 문장을 토큰(단어)를 분리하는 토크 나이저\n",
    "import string                                       \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9433af21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{33: None,\n",
       " 34: None,\n",
       " 35: None,\n",
       " 36: None,\n",
       " 37: None,\n",
       " 38: None,\n",
       " 39: None,\n",
       " 40: None,\n",
       " 41: None,\n",
       " 42: None,\n",
       " 43: None,\n",
       " 44: None,\n",
       " 45: None,\n",
       " 46: None,\n",
       " 47: None,\n",
       " 58: None,\n",
       " 59: None,\n",
       " 60: None,\n",
       " 61: None,\n",
       " 62: None,\n",
       " 63: None,\n",
       " 64: None,\n",
       " 91: None,\n",
       " 92: None,\n",
       " 93: None,\n",
       " 94: None,\n",
       " 95: None,\n",
       " 96: None,\n",
       " 123: None,\n",
       " 124: None,\n",
       " 125: None,\n",
       " 126: None}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str.maketrans(\"\", \"\", string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3a9bc3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'자연어 처리는 데이터 과학의 한 분야다 여러 전처리가 필요하다'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords                   # 불용어 목록\n",
    "from nltk.tokenize import word_tokenize             # 문장을 토큰(단어)를 분리하는 토크 나이저\n",
    "import string                                       \n",
    "import re\n",
    "\n",
    "# 원시 텍스트\n",
    "raw_text = \"자연어 처리는 데이터 과학의 한 분야다! 여러 전처리가 필요하다.\"\n",
    "\n",
    "# 특수문자 제거\n",
    "clean_text = raw_text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "# 숫자 제거\n",
    "clean_text = re.sub(r\"\\\\d+\", \"\", clean_text)\n",
    "\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca2c552f",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No such file or directory: 'C:\\\\Users\\\\Playdata\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\stopwords\\\\korean'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 불용어 제거\u001b[39;00m\n\u001b[32m      2\u001b[39m tokens = word_tokenize(clean_text)                                         \u001b[38;5;66;03m# 문장을 토큰 리스트로 토큰화\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m stop_words = \u001b[38;5;28mset\u001b[39m(\u001b[43mstopwords\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mkorean\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)                               \n\u001b[32m      4\u001b[39m filtered_tokens = [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]        \u001b[38;5;66;03m# 불용어에 해당하지 않는 토큰만 남긴다\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m전처리 결과:\u001b[39m\u001b[33m\"\u001b[39m, filtered_tokens)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\nlp\\nlp_venv\\Lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py:21\u001b[39m, in \u001b[36mWordListCorpusReader.words\u001b[39m\u001b[34m(self, fileids, ignore_lines_startswith)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwords\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileids=\u001b[38;5;28;01mNone\u001b[39;00m, ignore_lines_startswith=\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m     20\u001b[39m         line\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m line_tokenize(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileids\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     22\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line.startswith(ignore_lines_startswith)\n\u001b[32m     23\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\nlp\\nlp_venv\\Lib\\site-packages\\nltk\\corpus\\reader\\api.py:218\u001b[39m, in \u001b[36mCorpusReader.raw\u001b[39m\u001b[34m(self, fileids)\u001b[39m\n\u001b[32m    216\u001b[39m contents = []\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fileids:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[32m    219\u001b[39m         contents.append(fp.read())\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m concat(contents)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\nlp\\nlp_venv\\Lib\\site-packages\\nltk\\corpus\\reader\\api.py:231\u001b[39m, in \u001b[36mCorpusReader.open\u001b[39m\u001b[34m(self, file)\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    224\u001b[39m \u001b[33;03mReturn an open stream that can be used to read the given file.\u001b[39;00m\n\u001b[32m    225\u001b[39m \u001b[33;03mIf the file's encoding is not None, then the stream will\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    228\u001b[39m \u001b[33;03m:param file: The file identifier of the file to read.\u001b[39;00m\n\u001b[32m    229\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    230\u001b[39m encoding = \u001b[38;5;28mself\u001b[39m.encoding(file)\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_root\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m.open(encoding)\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\nlp\\nlp_venv\\Lib\\site-packages\\nltk\\data.py:333\u001b[39m, in \u001b[36mFileSystemPathPointer.join\u001b[39m\u001b[34m(self, fileid)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mjoin\u001b[39m(\u001b[38;5;28mself\u001b[39m, fileid):\n\u001b[32m    332\u001b[39m     _path = os.path.join(\u001b[38;5;28mself\u001b[39m._path, fileid)\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFileSystemPathPointer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\nlp\\nlp_venv\\Lib\\site-packages\\nltk\\data.py:311\u001b[39m, in \u001b[36mFileSystemPathPointer.__init__\u001b[39m\u001b[34m(self, _path)\u001b[39m\n\u001b[32m    309\u001b[39m _path = os.path.abspath(_path)\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(_path):\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo such file or directory: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m % _path)\n\u001b[32m    312\u001b[39m \u001b[38;5;28mself\u001b[39m._path = _path\n",
      "\u001b[31mOSError\u001b[39m: No such file or directory: 'C:\\\\Users\\\\Playdata\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\stopwords\\\\korean'"
     ]
    }
   ],
   "source": [
    "# 불용어 제거\n",
    "tokens = word_tokenize(clean_text)                                         # 문장을 토큰 리스트로 토큰화\n",
    "stop_words = set(stopwords.words(\"korean\"))                               \n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]        # 불용어에 해당하지 않는 토큰만 남긴다\n",
    "\n",
    "print(\"전처리 결과:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b331ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konlpy\n",
      "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting JPype1>=0.7.0 (from konlpy)\n",
      "  Downloading jpype1-1.6.0-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting lxml>=4.1.0 (from konlpy)\n",
      "  Downloading lxml-6.0.2-cp312-cp312-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\nlp\\nlp_venv\\lib\\site-packages (from konlpy) (2.4.1)\n",
      "Requirement already satisfied: packaging in c:\\nlp\\nlp_venv\\lib\\site-packages (from JPype1>=0.7.0->konlpy) (25.0)\n",
      "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
      "   ---------------------------------------- 0.0/19.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/19.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.8/19.4 MB 2.4 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 1.6/19.4 MB 2.8 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 2.1/19.4 MB 3.1 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 2.1/19.4 MB 3.1 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 2.4/19.4 MB 2.1 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 2.6/19.4 MB 1.8 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 2.6/19.4 MB 1.8 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 2.6/19.4 MB 1.8 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 2.6/19.4 MB 1.8 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 2.6/19.4 MB 1.8 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 2.6/19.4 MB 1.8 MB/s eta 0:00:10\n",
      "   ----- ---------------------------------- 2.9/19.4 MB 1.0 MB/s eta 0:00:16\n",
      "   ----- ---------------------------------- 2.9/19.4 MB 1.0 MB/s eta 0:00:16\n",
      "   ------ --------------------------------- 3.1/19.4 MB 951.4 kB/s eta 0:00:18\n",
      "   ------- -------------------------------- 3.4/19.4 MB 972.7 kB/s eta 0:00:17\n",
      "   ------- -------------------------------- 3.4/19.4 MB 972.7 kB/s eta 0:00:17\n",
      "   ------- -------------------------------- 3.4/19.4 MB 972.7 kB/s eta 0:00:17\n",
      "   ------- -------------------------------- 3.7/19.4 MB 886.5 kB/s eta 0:00:18\n",
      "   ------- -------------------------------- 3.7/19.4 MB 886.5 kB/s eta 0:00:18\n",
      "   ------- -------------------------------- 3.7/19.4 MB 886.5 kB/s eta 0:00:18\n",
      "   -------- ------------------------------- 3.9/19.4 MB 824.2 kB/s eta 0:00:19\n",
      "   -------- ------------------------------- 3.9/19.4 MB 824.2 kB/s eta 0:00:19\n",
      "   -------- ------------------------------- 3.9/19.4 MB 824.2 kB/s eta 0:00:19\n",
      "   -------- ------------------------------- 3.9/19.4 MB 824.2 kB/s eta 0:00:19\n",
      "   -------- ------------------------------- 3.9/19.4 MB 824.2 kB/s eta 0:00:19\n",
      "   -------- ------------------------------- 3.9/19.4 MB 824.2 kB/s eta 0:00:19\n",
      "   -------- ------------------------------- 3.9/19.4 MB 824.2 kB/s eta 0:00:19\n",
      "   -------- ------------------------------- 3.9/19.4 MB 824.2 kB/s eta 0:00:19\n",
      "   -------- ------------------------------- 3.9/19.4 MB 824.2 kB/s eta 0:00:19\n",
      "   -------- ------------------------------- 3.9/19.4 MB 824.2 kB/s eta 0:00:19\n",
      "   -------- ------------------------------- 4.2/19.4 MB 604.9 kB/s eta 0:00:26\n",
      "   --------- ------------------------------ 4.5/19.4 MB 625.7 kB/s eta 0:00:24\n",
      "   --------- ------------------------------ 4.7/19.4 MB 643.8 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 4.7/19.4 MB 643.8 kB/s eta 0:00:23\n",
      "   --------- ------------------------------ 4.7/19.4 MB 643.8 kB/s eta 0:00:23\n",
      "   ---------- ----------------------------- 5.0/19.4 MB 613.8 kB/s eta 0:00:24\n",
      "   ---------- ----------------------------- 5.2/19.4 MB 631.2 kB/s eta 0:00:23\n",
      "   ----------- ---------------------------- 5.5/19.4 MB 646.5 kB/s eta 0:00:22\n",
      "   ----------- ---------------------------- 5.8/19.4 MB 668.6 kB/s eta 0:00:21\n",
      "   ----------- ---------------------------- 5.8/19.4 MB 668.6 kB/s eta 0:00:21\n",
      "   ----------- ---------------------------- 5.8/19.4 MB 668.6 kB/s eta 0:00:21\n",
      "   ----------- ---------------------------- 5.8/19.4 MB 668.6 kB/s eta 0:00:21\n",
      "   ----------- ---------------------------- 5.8/19.4 MB 668.6 kB/s eta 0:00:21\n",
      "   ----------- ---------------------------- 5.8/19.4 MB 668.6 kB/s eta 0:00:21\n",
      "   ------------ --------------------------- 6.0/19.4 MB 599.2 kB/s eta 0:00:23\n",
      "   ------------ --------------------------- 6.0/19.4 MB 599.2 kB/s eta 0:00:23\n",
      "   ------------ --------------------------- 6.0/19.4 MB 599.2 kB/s eta 0:00:23\n",
      "   ------------ --------------------------- 6.3/19.4 MB 592.7 kB/s eta 0:00:23\n",
      "   -------------- ------------------------- 6.8/19.4 MB 627.9 kB/s eta 0:00:21\n",
      "   --------------- ------------------------ 7.6/19.4 MB 691.9 kB/s eta 0:00:18\n",
      "   ---------------- ----------------------- 7.9/19.4 MB 705.1 kB/s eta 0:00:17\n",
      "   ------------------- -------------------- 9.4/19.4 MB 825.9 kB/s eta 0:00:13\n",
      "   ------------------- -------------------- 9.7/19.4 MB 848.3 kB/s eta 0:00:12\n",
      "   ------------------- -------------------- 9.7/19.4 MB 848.3 kB/s eta 0:00:12\n",
      "   -------------------- ------------------- 10.0/19.4 MB 840.0 kB/s eta 0:00:12\n",
      "   -------------------- ------------------- 10.0/19.4 MB 840.0 kB/s eta 0:00:12\n",
      "   -------------------- ------------------- 10.0/19.4 MB 840.0 kB/s eta 0:00:12\n",
      "   -------------------- ------------------- 10.0/19.4 MB 840.0 kB/s eta 0:00:12\n",
      "   -------------------- ------------------- 10.0/19.4 MB 840.0 kB/s eta 0:00:12\n",
      "   --------------------- ------------------ 10.2/19.4 MB 778.4 kB/s eta 0:00:12\n",
      "   --------------------- ------------------ 10.5/19.4 MB 790.3 kB/s eta 0:00:12\n",
      "   ---------------------- ----------------- 10.7/19.4 MB 793.2 kB/s eta 0:00:11\n",
      "   ------------------------- -------------- 12.6/19.4 MB 921.2 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 13.1/19.4 MB 943.9 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 13.1/19.4 MB 943.9 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 13.6/19.4 MB 951.8 kB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 14.2/19.4 MB 975.0 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 15.5/19.4 MB 1.1 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 16.0/19.4 MB 1.1 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 16.3/19.4 MB 1.1 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 16.3/19.4 MB 1.1 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 16.5/19.4 MB 1.1 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 16.8/19.4 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 17.0/19.4 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 17.3/19.4 MB 1.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 17.3/19.4 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 17.6/19.4 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 17.6/19.4 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 17.8/19.4 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 18.1/19.4 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 18.1/19.4 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 18.4/19.4 MB 1.0 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 18.6/19.4 MB 1.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 18.6/19.4 MB 1.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 18.6/19.4 MB 1.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 18.9/19.4 MB 1.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 18.9/19.4 MB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  19.1/19.4 MB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 19.4/19.4 MB 1.0 MB/s  0:00:19\n",
      "Downloading jpype1-1.6.0-cp312-cp312-win_amd64.whl (355 kB)\n",
      "Downloading lxml-6.0.2-cp312-cp312-win_amd64.whl (4.0 MB)\n",
      "   ---------------------------------------- 0.0/4.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/4.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/4.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.5/4.0 MB 985.5 kB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 0.5/4.0 MB 985.5 kB/s eta 0:00:04\n",
      "   ------- -------------------------------- 0.8/4.0 MB 882.6 kB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 1.0/4.0 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 1.3/4.0 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 1.8/4.0 MB 1.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 2.1/4.0 MB 1.2 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 2.6/4.0 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 3.1/4.0 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 3.1/4.0 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 3.1/4.0 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.7/4.0 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.0/4.0 MB 1.4 MB/s  0:00:03\n",
      "Installing collected packages: lxml, JPype1, konlpy\n",
      "\n",
      "   ---------------------------------------- 0/3 [lxml]\n",
      "   ---------------------------------------- 0/3 [lxml]\n",
      "   ---------------------------------------- 0/3 [lxml]\n",
      "   ---------------------------------------- 0/3 [lxml]\n",
      "   ---------------------------------------- 0/3 [lxml]\n",
      "   ------------- -------------------------- 1/3 [JPype1]\n",
      "   ------------- -------------------------- 1/3 [JPype1]\n",
      "   ------------- -------------------------- 1/3 [JPype1]\n",
      "   ------------- -------------------------- 1/3 [JPype1]\n",
      "   ------------- -------------------------- 1/3 [JPype1]\n",
      "   -------------------------- ------------- 2/3 [konlpy]\n",
      "   -------------------------- ------------- 2/3 [konlpy]\n",
      "   -------------------------- ------------- 2/3 [konlpy]\n",
      "   -------------------------- ------------- 2/3 [konlpy]\n",
      "   ---------------------------------------- 3/3 [konlpy]\n",
      "\n",
      "Successfully installed JPype1-1.6.0 konlpy-0.6.0 lxml-6.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e53ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f5af6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e06798e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['자연어', '처리', '데이터', '과학', '의', '분야', '다', '여러', '전', '처리', '필요하다']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "import re, string\n",
    "\n",
    "\n",
    "okt = Okt(jvmpath=r\"C:\\Program Files\\Java\\jdk-21\\bin\\server\\jvm.dll\")\n",
    "\n",
    "# 원시 텍스트\n",
    "raw_text = \"자연어 처리는 데이터 과학의 한 분야다! 여러 전처리가 필요하다.\"\n",
    "\n",
    "# 특수문자 제거\n",
    "clean_text = raw_text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "# 숫자 제거\n",
    "clean_text = re.sub(r\"\\\\d+\", \"\", clean_text)\n",
    "\n",
    "tokens = okt.morphs(clean_text)     # 형태소 단위 토큰화\n",
    "\n",
    "stop_words = set = {\"은\", \"는\", \"이\", \"가\" , \"을\", \"를\", \"에\", \"에서\", \"와\", \"과\", \"도\", \"한\", \"하다\", \" 되다\", \"있다\", \"없디\"}                      \n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]        # 불용어에 해당하지 않는 토큰만 남긴다\n",
    "filtered_tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d142c5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "형태소 분석 결과: ['자연어', '처리', '는', '정말', '재미있다', '!']\n",
      "품사 태깅 결과: [('자연어', 'Noun'), ('처리', 'Noun'), ('는', 'Josa'), ('정말', 'Noun'), ('재미있다', 'Adjective'), ('!', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "# 텍스트\n",
    "text = \"자연어 처리는 정말 재미있다!\"\n",
    "\n",
    "# 형태소 분석\n",
    "okt = Okt()\n",
    "morphs = okt.morphs(text)       # 문장을 형태소(최소 의미 단위) 리스트로 분리\n",
    "pos = okt.pos(text)             # (형태소, 품사) 튜플 리스트로 품사 태깅 수행\n",
    "\n",
    "print(\"형태소 분석 결과:\", morphs)\n",
    "print(\"품사 태깅 결과:\", pos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
