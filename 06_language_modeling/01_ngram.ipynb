{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "830da992",
   "metadata": {},
   "source": [
    "# n-gram 언어 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05227122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0167bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"오늘은 날씨가 좋다. 오늘은 기분이 좋다. 오늘은 일이 많다. 오늘은 사람이 많다. 오늘은 날씨가 맑다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81cbb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(text)   # 단어 단위로 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16ce1e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'오늘은': 5, '.': 5, '날씨가': 2, '좋다': 2, '많다': 2, '기분이': 1, '일이': 1, '사람이': 1, '맑다': 1})\n",
      "Counter({('.', '오늘은'): 4, ('오늘은', '날씨가'): 2, ('좋다', '.'): 2, ('많다', '.'): 2, ('날씨가', '좋다'): 1, ('오늘은', '기분이'): 1, ('기분이', '좋다'): 1, ('오늘은', '일이'): 1, ('일이', '많다'): 1, ('오늘은', '사람이'): 1, ('사람이', '많다'): 1, ('날씨가', '맑다'): 1, ('맑다', '.'): 1})\n"
     ]
    }
   ],
   "source": [
    "# 1-gram, 2-gram\n",
    "unigram = tokens\n",
    "bigram = list(ngrams(tokens, 2))\n",
    "\n",
    "unigram_freq = Counter(unigram)\n",
    "bigram_freq = Counter(bigram)\n",
    "print(unigram_freq)\n",
    "print(bigram_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e34ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(날씨가|오늘은) = 0.400\n",
      "P(좋다|날씨가) = 0.500\n",
      "P(.|좋다) = 1.000\n",
      "P(오늘은|.) = 0.800\n",
      "P(기분이|오늘은) = 0.200\n",
      "P(좋다|기분이) = 1.000\n",
      "P(일이|오늘은) = 0.200\n",
      "P(많다|일이) = 1.000\n",
      "P(.|많다) = 1.000\n",
      "P(사람이|오늘은) = 0.200\n",
      "P(많다|사람이) = 1.000\n",
      "P(맑다|날씨가) = 0.500\n",
      "P(.|맑다) = 1.000\n"
     ]
    }
   ],
   "source": [
    "# Bigram 조건부 확률 계산 및 출력\n",
    "for (w1, w2), freq in bigram_freq.items():  \n",
    "    prob = freq / unigram_freq[w1]          # 조건부 확률 계산 (w1 뒤에 w2가 올 확률)\n",
    "    print(f'P({w2}|{w1}) = {prob:.3f}')     # w1 다음에 w2가 등장할 확률"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3fbda4",
   "metadata": {},
   "source": [
    "### Perplexity 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d0a5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Perplexity의 평가 기준\n",
    "# - 모델이 테스트 데이터에서 얼마나 적은 불확실성을 가지며 다음 단어를 잘 예측하는가\n",
    "\n",
    "# 테스트 문장으로부터 바이그램 확률을 이용해 perplexity(혼란도)를 계산하는 함수\n",
    "def compute_bigram_perplexity(test_text, unigram_freq, bigram_freq):\n",
    "    test_tokens = nltk.word_tokenize(test_text) # 테스트 텍스트 토큰화\n",
    "    test_bigrams = list(ngrams(test_tokens, 2)) # 테스트 문장 바이그램\n",
    "\n",
    "    log_prob_sum = 0                            # 로그 확률 합 누적\n",
    "    N = len(test_bigrams)                       # 평균용 바이그램 개수\n",
    "\n",
    "    for bigram in test_bigrams:\n",
    "        w1, w2 = bigram\n",
    "        prob = bigram_freq.get(bigram, 0) / unigram_freq.get(w1, 1) # P(w2|w1) 계산\n",
    "        if prob == 0:                           # 확률이 0이면\n",
    "            prob = 1e-10                        # 매우 미세한 값으로 대체 (오류방지용)\n",
    "        log_prob_sum += math.log2(prob)         # log2 확률로 누적(교차 엔트로피 계산용)\n",
    "\n",
    "    cross_entropy = -log_prob_sum / N           # Cross-Entropy = -(1/N) * log_prob_sum\n",
    "    perplexity = math.pow(2, cross_entropy)     # Perplexity = 2^(cross_entropy)\n",
    "\n",
    "    return perplexity                         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5a260e",
   "metadata": {},
   "source": [
    "   perplexity : 값이 작을수록 테스트 문장을 더 잘 예측한 모델이다. (1 = 혼동성 없음, 5 = 매 순간 평균 5개중에 혼동됨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3676ce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = \"자연어 처리는 재미있다. 자연어 처리는 어렵지만 도전하고 싶다. 오늘은 날씨가 좋다.\"\n",
    "\n",
    "train_tokens = nltk.word_tokenize(train_text)\n",
    "\n",
    "unigram = train_tokens\n",
    "bigrams = list(ngrams(train_tokens, 2))\n",
    "\n",
    "unigram_freq = Counter(unigram)\n",
    "bigrams_freq = Counter(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a10c9213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "자연어 처리는 재미있다. perplexity: 1.2599210498948732\n",
      "자연어 처리는 어렵지만 도전하고 싶다. perplexity: 1.148698354997035\n",
      "오늘은 날씨가 좋다. perplexity: 1.0\n",
      "기계 번역은 어렵다. perplexity: 10000000000.000008\n",
      "자연어 처리에 도전하고 싶다. perplexity: 100000.00000000003\n",
      "오늘 날씨가 흐리다. perplexity: 10000000000.000008\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"자연어 처리는 재미있다.\",\n",
    "    \"자연어 처리는 어렵지만 도전하고 싶다.\",\n",
    "    \"오늘은 날씨가 좋다.\",\n",
    "    \"기계 번역은 어렵다.\",\n",
    "    \"자연어 처리에 도전하고 싶다.\",\n",
    "    \"오늘 날씨가 흐리다.\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    pp = compute_bigram_perplexity(sentence, unigram_freq, bigrams_freq)\n",
    "    print(sentence, \"perplexity:\", pp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb702baf",
   "metadata": {},
   "source": [
    "학습 문장에 나왔던 표현은 상대적으로 낮게 (1에 가깝게) 나오는 반면,  \n",
    "학습에 없던 조합은 지표가 높게 나온다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
